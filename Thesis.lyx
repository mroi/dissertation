#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass tufte-book
\begin_preamble
% simplified title and author for fancy headers, height-correct the dash
\renewcommand{\plaintitle}{Practical Real\raisebox{-0.45mm}{-}Time with Look\raisebox{-0.45mm}{-}Ahead Scheduling}
\renewcommand{\plainauthor}{Michael Roitzsch}

% typesetting tweaks
\clubpenalty=9999
\widowpenalty=9999
\relpenalty=9999
\binoppenalty=9999
\hyphenation{Atlas}

% font setup: disable Hoefler swashes, set math font
\usepackage[MnSymbol]{mathspec}
\setmainfont[ItalicFeatures={Contextuals={NoLineInitial,NoLineFinal}}]{Hoefler Text}
\setmathsfont(Digits,Latin)[ItalicFeatures={Contextuals={NoLineInitial,NoLineFinal}},Numbers=Lining]{Hoefler Text}
\setmathsfont(Greek){Georgia}
% restore mathspec's activation of " at document beginning, someone messes with it
\begingroup
\catcode`\"=\active
\AtBeginDocument{\let"=\eu@active@quote}
\endgroup

% letter spacing for capitalized text
\renewcommand{\allcapsspacing}[1]{{\addfontfeature{LetterSpace=6.5}#1}}
\renewcommand{\smallcapsspacing}[1]{{\addfontfeature{LetterSpace=5.0,Letters=SmallCaps}#1}}

% use all-bullets for itemize and give them the right size and height
\AtBeginDocument{
\def\labelitemi{\raisebox{0.3mm}{\scriptsize\(\bullet\)}}
\def\labelitemii{\raisebox{0.3mm}{\scriptsize\(\bullet\)}}
\def\labelitemiii{\raisebox{0.3mm}{\scriptsize\(\bullet\)}}
\def\labelitemiv{\raisebox{0.3mm}{\scriptsize\(\bullet\)}}
}

% do not indent the bibliography
\setlength{\bibhang}{0pt}

% repeat tufte caption definition because it gets garbled by hyperref
\long\def\@caption#1[#2]#3{\par%
\addcontentsline{\csname ext@#1\endcsname}{#1}%
{\protect\numberline{\csname the#1\endcsname}{\ignorespaces #2}}%
\begingroup%
\@parboxrestore%
\if@minipage\@setminipage\fi%
\@tufte@caption@font\@tufte@caption@justification%
\noindent\csname fnum@#1\endcsname: \ignorespaces#3\par%
\endgroup}

% colored page edge when a new chapter starts
\newcommand{\chapteredge}{
\fancypagestyle{plain}{
\fancyhf{}
\fancyfoot[LE,RO]{
\begin{picture}(0,0)
\color{gray}
\put(49,-60){\rule{10mm}{305mm}}
\end{picture}
}}}
\newcommand{\nochapteredge}{
\fancypagestyle{plain}{
\fancyhf{}
}}

% alternative chapter titling with big chapter number on the right
\titleformat{\chapter}[display]
{\relax\ifthenelse{\NOT\boolean{@tufte@symmetric}}{\begin{fullwidth}}{}}
{\hfill\sffamily\color{gray}\fontsize{42}{0}\selectfont\thechapter}
{0pt}
{\vskip -32pt\huge\rmfamily\itshape}
[\ifthenelse{\NOT\boolean{@tufte@symmetric}}{\end{fullwidth}}{}]

% epigraph command
\newcommand{\epigraph}[2]{
\begin{fullwidth}
\sffamily\Large
\begin{doublespace}
\noindent\allcaps{#1}\\% epigraph
\color{darkgray}\noindent\allcaps{#2}% author
\end{doublespace}
\end{fullwidth}
}
\end_preamble
\options a4paper
\use_default_options true
\begin_modules
fixltx2e
enumitem
\end_modules
\maintain_unincluded_children false
\begin_local_layout
Style Verleger
Margin	Static
LatexType	Command
Category	FrontMatter
LatexName	publisher
InTitle	1
InPreamble	1
Font
Size	Large
EndFont
End
\end_local_layout
\language english
\language_package default
\inputencoding utf8-plain
\fontencoding global
\font_roman Hoefler Text
\font_sans Gill Sans
\font_typewriter Menlo Regular
\font_default_family default
\use_non_tex_fonts true
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 83

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_title "Practical Real-Time with Look-Ahead Scheduling"
\pdf_author "Michael Roitzsch"
\pdf_subject "Dissertation"
\pdf_keywords "Real-Time, Multicore, Work Queues, Look-Ahead, Clairvoyance"
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 2
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 0
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
frontmatter
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
null
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
epigraph{I love deadlines.
\backslash
newline
\backslash
noindent I love the whooshing noise they make as they go by.}{Douglas Adams}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
epigraph{The art of prophecy is very difficult,
\backslash
newline
\backslash
noindent especially with respect to the future.}{Mark Twain}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
epigraph{Oh dear! Oh dear! I shall be too late!}{The White Rabbit}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage cleardoublepage
\end_inset


\end_layout

\begin_layout Author
Michael Roitzsch – born August 15, 1980
\end_layout

\begin_layout Title
Practical Real
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
raisebox{0.8mm}{-}
\end_layout

\end_inset

Time
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

with Look
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
raisebox{0.8mm}{-}
\end_layout

\end_inset

Ahead
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

Scheduling
\end_layout

\begin_layout Verleger
Dissertation
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

Advisor: Prof.
 Dr.
 rer.
 nat.
 Hermann Härtig
\begin_inset Newline newline
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

Technische Universität Dresden
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
null
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset


\begin_inset Graphics
	filename Figures/TU-Logo.pdf
	width 5cm

\end_inset


\end_layout

\begin_layout Full Width

\lang ngerman
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout

\lang ngerman
Dissertation
\end_layout

\end_inset

 zur Erlangung des akademischen Grades 
\begin_inset Flex AllCaps
status collapsed

\begin_layout Plain Layout

\lang ngerman
Doktoringenieur
\begin_inset space ~
\end_inset

(Dr.-Ing.)
\end_layout

\end_inset

, vorgelegt an der 
\begin_inset Flex SmallCaps
status collapsed

\begin_layout Plain Layout

\lang ngerman
Technischen Universität Dresden, Fakultät Informatik
\end_layout

\end_inset

, eingereicht von 
\begin_inset Flex AllCaps
status collapsed

\begin_layout Plain Layout

\lang ngerman
Dipl.-Inf.
 Michael Roitzsch
\end_layout

\end_inset

, geboren am 15.
 August 1980 in Dresden.
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Full Width

\lang ngerman
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="2">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top" width="0pt" special="@{}l">
<column alignment="left" valignment="top" width="0pt">
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
Betreuender Hochschullehrer:
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
Prof.
 Dr.
 rer.
 nat.
 Hermann Härtig,
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
Technische Universität Dresden
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
Fachreferent:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
Prof.
 Dr.
 Christof Fetzer,
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
Technische Universität Dresden
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
Gutachter:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout

\lang ngerman
fehlt
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
Statusvortrag:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
12.
 Dezember 2011
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
Verteidigung:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\lang ngerman
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout

\lang ngerman
fehlt
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Full Width

\lang ngerman
\begin_inset VSpace bigskip
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

Dresden, 
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout

\lang ngerman
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Chapter*
Acknowledgments
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Plain Layout
OS group, Prof.
 Härtig, fellow researchers, friends, parents
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage cleardoublepage
\end_inset


\end_layout

\begin_layout Full Width
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
null
\end_layout

\end_inset


\begin_inset VSpace vfill
\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
noindent
\backslash
nohyphenation 
\end_layout

\end_inset


\shape italic
\size huge

\begin_inset Note Greyedout
status open

\begin_layout Plain Layout

\shape italic
\size huge
Dedication
\end_layout

\end_inset


\end_layout

\begin_layout Full Width
\begin_inset VSpace vfill
\end_inset


\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
mainmatter
\backslash
chapteredge
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
The computing revolution
\end_layout

\end_inset

 has treated users with an impressive stream of innovation: From the mainframe
 era through the age of productivity computing to today’s multimedia and
 mobile world, the capabilities of systems and thus the possibilities for
 users increased steadily.
 This development thrives on the exponential improvement of the underlying
 transistor technology as predicted by Moore’s Law
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
Moore’s Law
\end_layout

\end_inset

: The number of transistors that can be integrated cost efficiently doubles
 approximately every two years.
\begin_inset Foot
status open

\begin_layout Plain Layout
This period is often misquoted as 18 months, which is a follow-up prediction
 of the increase in single chip performance.
\end_layout

\end_inset

 Software matches this exponential growth: The total body of open source
 software in the world doubles about every 14 months.
\begin_inset CommandInset citation
LatexCommand cite
key "Deshpande:GrowthSoftware"

\end_inset


\end_layout

\begin_layout Section
Application Challenges
\end_layout

\begin_layout Standard
Users have grown accustomed to the constant improvement of the technology.
 What started as high-end and expert use cases will become a commodity just
 a few years later and will be expected to work predictably and efficiently.
 The present-day iPad
\begin_inset space ~
\end_inset

2 would have been in 1994’s Top
\begin_inset space ~
\end_inset

500 list of the fastest computers in the world
\begin_inset CommandInset citation
LatexCommand cite
key "Markoff:DongarraIPad"

\end_inset

 and it is arguably more accessible today than those supercomputers were
 back then.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
Satisfying these expectations
\end_layout

\end_inset

 is a demanding job for developers.
 Users are no longer satisfied with functionality alone.
 Increasingly, 
\begin_inset Flex SmallCaps
status collapsed

\begin_layout Plain Layout
non-functional properties
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
non-functional properties
\end_layout

\end_inset

 separate good from great applications.
 Next to user interface design and visual appearance such properties include
 responsive and stutter-free operation, perceived performance, and the useful
 and efficient employment of the invested resources, also driven by the
 resource and energy constraints of today’s battery-powered devices.
 The following three non-functional properties motivate this dissertation:
\end_layout

\begin_layout Description
Timeliness
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
timeliness
\end_layout

\end_inset

: User interface responsiveness and the smoothness of multimedia operations
 require that application’s 
\begin_inset Flex SmallCaps
status collapsed

\begin_layout Plain Layout
timing requirements
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
timing requirements
\end_layout

\end_inset

 are met.
\begin_inset CommandInset citation
LatexCommand cite
key "Sasinowski:ARTIFACT"

\end_inset

 Such requirements arise when computers interface with the real world.
 Subsumed under the term real-time
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
real-time
\end_layout

\end_inset

, they tie the completion of software results to wall-clock time.
 Time being a global resource, a system-wide solution is needed.
 However, the majority of commodity systems today offer only weak temporal
 guarantees and predictability to applications, so developers have to work
 around those limitations.
 While real-time operating systems are available, they are typically applied
 only in special-purpose scenarios.
\begin_inset Foot
status open

\begin_layout Plain Layout
A notable exception is the BlackBerry PlayBook
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
BlackBerry PlayBook
\end_layout

\end_inset

, a general-purpose tablet computer which runs the real-time capable QNX
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
QNX
\end_layout

\end_inset

 kernel.
\end_layout

\end_inset


\end_layout

\begin_layout Description
Efficient
\begin_inset space \space{}
\end_inset

Core
\begin_inset space \space{}
\end_inset

Placement: Single-core performance is leveling off, so processor manufacturers
 rely on increasing core counts to offer higher performance.
 Applications thus have to employ multiple threads to benefit from multiple
 cores.
\begin_inset CommandInset citation
LatexCommand cite
key "Sutter:FreeLunch"

\end_inset

 The amount of parallelism exposed by applications limits the number of
 cores occupied.
 However, the assignment of work to cores also influences cache contention
\begin_inset CommandInset citation
LatexCommand cite
key "Zhuravlev:DIO"

\end_inset

 and energy efficiency.
\begin_inset CommandInset citation
LatexCommand cite
key "Rangan:ThreadMotion"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
a paper on core consolidation to save energy would be better
\end_layout

\end_inset

 Both processors and energy are global resources, so they require system-wide
 management.
 Applications have only limited ways to manage these issues locally.
 It is preferable for applications to contribute their local knowledge of
 
\begin_inset Flex SmallCaps
status collapsed

\begin_layout Plain Layout
resource requirements
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
resource requirements
\end_layout

\end_inset


\emph on
 
\emph default
to improve the system's decisions.
\end_layout

\begin_layout Description
Quality-aware
\begin_inset space \space{}
\end_inset

Overload
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
overload
\end_layout

\end_inset


\begin_inset space \space{}
\end_inset

Handling: Overload situations occur when all ready applications collectively
 ask for more computation time than the machine can offer without violating
 any timing requirements.
 Real-time systems avoid such situations statically with an admission
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
admission
\end_layout

\end_inset

 process: The system will not allow new tasks unless it can establish a
 formal guarantee that the total resource demand will never outgrow the
 available resources.
 On interactive systems, users will not accept rejected application launches.
\begin_inset CommandInset citation
LatexCommand cite
key "Apple:UserControl"

\end_inset

 These systems have to handle overload situations dynamically at runtime
 by reducing the resource allotment of a subset of applications.
 To maintain fairness, this reduction needs global management, but applications
 may want to adapt their service according to a local notion of quality.
 The system should communicate the overload 
\begin_inset Flex SmallCaps
status collapsed

\begin_layout Plain Layout
ahead of time
\end_layout

\end_inset

,
\emph on
 
\emph default
so those applications can prepare before the resource shortage arrives.
\end_layout

\begin_layout Standard
These three challenges all benefit from the aggregation of local application
 knowledge to implement a global system-wide policy.
 I will show how communicating application’s timing requirements helps overall
 timeliness, how knowledge on resource requirements improves core placement
 and how ahead-of-time overload notification improves quality when adapting.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
Modern applications are complex
\end_layout

\end_inset

 in functionality because they offer intricate features or have to handle
 new failure cases by connecting with the cloud.
 To also deliver on non-functional properties, developers need to build
 on top of a foundation that helps them handle these problems without much
 development overhead.
 Libraries can mediate between the application’s view and the system interface.
 After discussing the application needs, I will now motivate my work from
 the system’s perspective.
\end_layout

\begin_layout Section
Scheduler Knowledge
\end_layout

\begin_layout Standard
\begin_inset Float marginfigure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Figures/1 Introduction/Interaction Application Scheduler.svg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "mar:1-Interaction"

\end_inset

Interaction between Application and Scheduler
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As illustrated by Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "mar:1-Interaction"

\end_inset

, the scheduler
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
scheduler
\end_layout

\end_inset

 is a system component responsible for accumulating the information about
 the work applications want to perform and determine an order and core placement
 to execute that work.
 The ordering policy should serve the non-functional properties applications
 expect: It should fulfill timing requirements, use cores efficiently and
 handle overload early and fairly.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
Because every device in a system
\end_layout

\end_inset

 has a different notion of work and a different policy for ordering work,
 each device has its own dedicated scheduler.
 However, to submit work to a peripheral device, applications need to execute
 code on the CPU.
 Running the scheduler for that peripheral also requires the execution of
 code on the CPU.
 So the CPU and its scheduling plays a central role in the system as it
 is the gateway for the scheduling of all other devices.
\begin_inset CommandInset citation
LatexCommand cite
key "Rajkumar:ResourceKernels"

\end_inset

 Even though I believe many of the ideas I am going to present here also
 apply to scheduling peripherals, this work focuses exclusively on scheduling
 the CPU cores of a computer system.
\end_layout

\begin_layout Standard
Nevertheless, taking a closer look at peripheral schedulers does help to
 reveal an important disadvantage of CPU scheduling: Schedulers for peripheral
 devices have a deeper insight into the jobs they are supposed to order.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
So how does a peripheral scheduler operate?
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
peripheral device scheduling
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Love:IO_Schedulers"

\end_inset

 The story starts with an application asking for the service of a device.
 Let us use a write request to persistent storage as an example.
 The application collects the data it wants to make durable and submits
 a job toward the device using an API or system call.
 A write request to a file would now traverse various layers of file system
 and buffer caching code in the operating system, but would finally arrive
 at the device driver as a write request to the magnetic disk.
 As part of the driver, the scheduler maintains a work queue of jobs waiting
 for execution.
 Our write request is added to that queue together with concurrent requests
 arriving from other applications, from other threads of the same application,
 or even from the same thread of the same application, if the initial write
 executes asynchronously.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "mar:1-Device-Scheduling"

\end_inset

 depicts this situation.
\end_layout

\begin_layout Standard
\begin_inset Float marginfigure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Figures/1 Introduction/Peripheral Scheduling.svg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "mar:1-Device-Scheduling"

\end_inset

Peripheral Device Scheduling
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The scheduler now orders the jobs and sends them off to the device for execution.
 The scheduling algorithm performs that ordering with a service goal in
 mind, for example to maximize device throughput.
 Every job carries metadata for the request.
 In the case of our disk write request, that metadata contains the size
 of the request and its location on disk.
\end_layout

\begin_layout Standard
When ordering jobs, the scheduler can inspect the metadata to decide which
 order best supports its service goal.
 The disk scheduler in the example can use the on-disk location to execute
 a shortest access time first
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
shortest access time first
\end_layout

\end_inset

 policy
\begin_inset CommandInset citation
LatexCommand cite
key "Jacobson:SATF"

\end_inset

 to improve drive throughput.
 Peripheral device schedulers enjoy the advantage that the outstanding requests
 are self-describing jobs
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
self-describing job
\end_layout

\end_inset

.
 Inspecting them reveals all information the peripheral will use to execute
 them.
 This is natural, because to program the device, the driver must have all
 that information available anyway.
 We will see next that CPU schedulers do not share this benefit.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
As a side note,
\end_layout

\end_inset

 even though this work does not deal with peripheral scheduling: What complicate
s the metadata-driven scheduling is the increasing complexity of devices
 themselves.
 The position of our disk write request is a logical block number, with
 only the drive knowing the physical position of the actual sector.
 Devices also increasingly develop internal self-scheduling capabilities:
 The software scheduler in the driver can send multiple outstanding requests
 to the device and a hardware scheduler in the device will use its more
 detailed knowledge on request execution properties to order the requests
 beneficially.
\end_layout

\begin_layout Standard
Disks behave this way since the introduction of native command queueing
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
native command queueing
\end_layout

\end_inset

.
\begin_inset CommandInset citation
LatexCommand cite
key "Intel:NCQ"

\end_inset

 But because the device’s scheduling policy is fixed, it still makes sense
 for the software scheduler to pre-order the request according to its own
 policy, using job metadata derived with a simplified model of the device.
 A disk scheduler may have a notion of request urgency and inter-application
 fairness that the hardware scheduler would not know about.
\begin_inset CommandInset citation
LatexCommand cite
key "Reuther:DAS"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float marginfigure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\begin_inset Graphics
	filename Figures/1 Introduction/CPU Scheduling.svg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "mar:1-CPU-Scheduling"

\end_inset

CPU Scheduling
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset VSpace defskip
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
Turning to the CPU scheduler
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
CPU scheduling
\end_layout

\end_inset

, we notice an important difference: It does not deal with self-describing
 jobs in a queue of outstanding work.
 Instead, it maintains a ready queue of runnable threads
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
thread
\end_layout

\end_inset

.
 Other than a job, a thread is not consumed after it executed, but is automatica
lly inserted into the ready queue again until terminated by the application.
 Other than a job, a thread is not a self-describing aggregate of payload
 and metadata, but rather an opaque handle to an execution context within
 the application’s address space.
 The actual behavior that unfolds if the thread is executed
\begin_inset space \thinspace{}
\end_inset

—
\begin_inset space \thinspace{}
\end_inset

whether it blocks after a short burst of code, or runs a long computation,
 or a periodic task
\begin_inset space \thinspace{}
\end_inset

—
\begin_inset space \thinspace{}
\end_inset

is hidden within the application’s code and memory state as illustrated
 by Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "mar:1-CPU-Scheduling"

\end_inset

.
\end_layout

\begin_layout Standard
CPU schedulers in commodity operating systems support a notion of precedence,
 expressed with priorities
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
priority
\end_layout

\end_inset

 or the Unix nice levels
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
nice level
\end_layout

\end_inset

.
 This single numeric value
\begin_inset CommandInset citation
LatexCommand cite
key "Unix:nice"

\end_inset

 is a coarse abstraction of a thread’s behavior, because it only indicates
 the importance of a thread relative to other threads.
 The application developer has to supply the priority without knowledge
 of concurrent load, which gives rise to other problems we discuss in Chapter
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "chap:3-Real-Time"

\end_inset

.
\end_layout

\begin_layout Standard
Real-time schedulers supporting a periodic task model
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
periodic task model
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Liu:RMS"

\end_inset

 have the benefit of implicit knowledge about a thread’s upcoming behavior,
 but only for applications fitting into that rigid model.
 More details again follow in Chapter
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "chap:3-Real-Time"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
In this thesis
\end_layout

\end_inset

, I want to argue that it is beneficial to organize CPU scheduling using
 self-describing jobs similar to peripheral schedulers.
 Those CPU jobs represent a bounded piece of code execution and are consumed
 once the CPU executed them.
 They hold metadata such as timing requirements to indicate urgency and
 resource requirements to indicate the execution time needed.
 If the CPU scheduler receives jobs ahead-of-time, before they execute,
 it can build up a limited look into the applications’ future and detect
 overload situations early.
 I show how such a scheduling regime improves the non-functional application
 properties portrayed above.
\end_layout

\begin_layout Section
Driving Insights
\end_layout

\begin_layout Standard
The walk-through of the problem area shows that the considered non-functional
 properties timeliness, efficient core placement, and quality-aware overload
 handling are cross-cutting concerns.
 They involve local knowledge from the applications and global policy executed
 by the scheduler.
 Furthermore, many applications act as an execution environment tailored
 to a specific workload
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
workload
\end_layout

\end_inset

: a text editor manipulates documents, a photo album manages photos, a video
 player renders continuous media.
 Those applications’ behavior dynamically depends on the workload they handle.
 This workload is what users care about after all.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
A vertically integrated solution
\end_layout

\end_inset

 spanning from the workload through the application down to the scheduler
 is called for.
\end_layout

\begin_layout Standard
\begin_inset Float marginfigure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Figures/1 Introduction/Verically Integrated Solution.svg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "mar:1-Vertically-Integrated-Solution"

\end_inset

Vertically Integrated Solution
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
null
\end_layout

\end_inset


\end_layout

\end_inset

Refining the previous concept sketch from Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "mar:1-Interaction"

\end_inset

, Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "mar:1-Vertically-Integrated-Solution"

\end_inset

 shows how I imagine this integration to operate.
 Applications should be aware of their workload and extract information
 to model it.
 They communicate a useful representation of their local knowledge down
 to the scheduler.
 The scheduler collects this knowledge from all applications and executes
 its global policy, consequences of this policy take effect either implicitly
 by executing the scheduling decisions, or explicitly by the scheduler reporting
 back to an application.
 Overload situations propagate this way, because they require an application-spe
cific notion of quality to resolve them.
 The application in turn uses its model of the workload to decide on a quality-a
ware reaction.
\end_layout

\begin_layout Standard
What information is relayed along those paths and what the interfaces look
 like remains to be discussed.
 But as motivated in the section on scheduler knowledge above, the scheduler
 should operate on self-contained jobs instead of threads which hide their
 execution behavior deep in the application state.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
Applications should expose local knowledge
\end_layout

\end_inset

 to the scheduler using self-describing jobs that encapsulate timing and
 resource requirements depending on the current workload.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
null
\end_layout

\end_inset


\end_layout

\end_inset

To expose more knowledge than currently, applications likely have to be
 modified.
 We may get away with adapting key libraries to change the behavior of many
 applications at once, but in the context of this dissertation, modifying
 a library counts as modifying the application.
 How to reduce programming effort by architecting software so it hides this
 problem in library layers is outside the scope of this thesis.
 In scope however is to design the interfaces for ease of use:
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
The programming model should be approachable
\end_layout

\end_inset

 by matching current application development methods and by never asking
 the developer for parameters outside the application domain.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
null
\end_layout

\end_inset


\end_layout

\end_inset

I hope developers can provide parameters from within the application domain
 with reasonable effort.
 An example are deadlines
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
deadlines
\end_layout

\end_inset

 to describe the timing requirements of a job.
 Parameters requiring knowledge outside the application scope should be
 avoided.
 Estimated execution times of jobs are specific to the underlying hardware
 platform, so developers should not need to provide those.
\end_layout

\begin_layout Standard
An emerging trend in the development of parallel software is the use of
 lambdas
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
lambda
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Sutter:LambdasEverywhere"

\end_inset

, depending on the programming language also called closures
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
closure
\end_layout

\end_inset

 or blocks
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
block
\end_layout

\end_inset

.
 They are used to structure code into pieces that can be executed asynchronously
, which keeps applications responsive in the presence of long-running background
 computation or IO
\begin_inset space ~
\end_inset

operations.
 Because asynchronously executed code runs concurrently with other code,
 lambdas are also a tool to express parallelism.
 A more in-depth discussion of this programming style follows in Chapter
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "chap:2-Anatomy"

\end_inset

.
 I will illustrate how lambdas and jobs can cooperate
\begin_inset space \thinspace{}
\end_inset

—
\begin_inset space \thinspace{}
\end_inset

both describe a bounded, self-contained piece of code execution
\begin_inset space \thinspace{}
\end_inset

—
\begin_inset space \thinspace{}
\end_inset

and how the communication of jobs to the scheduler integrates with the use
 of lambdas.
 Big platform vendors such as Apple
\begin_inset CommandInset citation
LatexCommand cite
key "Apple:GrandCentralDispatch"

\end_inset

 and Microsoft
\begin_inset CommandInset citation
LatexCommand cite
key "MSDN:C++PPL"

\end_inset

 adopted lambda programming, so by drafting a scheduler interface that leans
 toward it, I intend to create a pragmatic solution that is manageable by
 developers.
\end_layout

\begin_layout Standard
The asynchronous execution of lambdas gives rise to another interesting
 benefit: announcing future code execution ahead-of-time.
 Application code can specify multiple pieces of work and dispatch them
 for later, asynchronous and potentially parallel execution.
 The queues in the lambda runtime therefore contain knowledge about what
 pieces of code will execute in the future.
 Again, more details on how these runtimes operate follow in Chapter
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "chap:2-Anatomy"

\end_inset

.
 Here we conclude:
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
Knowledge of future execution
\end_layout

\end_inset

 should be propagated to the scheduler.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
null
\end_layout

\end_inset


\end_layout

\end_inset

I claim that many applications have knowledge on their future execution
 available, but currently lack a way to expose this information.
 Any operation that runs autonomously after being triggered by a system
 event or user interaction is fully determined at the moment it is triggered.
 If a user clicks 
\begin_inset Quotes eld
\end_inset

play
\begin_inset Quotes erd
\end_inset

 to watch a video, the application knows that it will now be fetching, decoding
 and displaying video frames.
\end_layout

\begin_layout Standard
Like video, some of these chains of actions may be long-running, others
 may be short, like the reaction to a user’s mouse click in a graphical
 interface.
 Highly interactive applications like games may have almost no knowledge
 of what will happen next, because the user can change the course of action
 at any moment.
 However, applications that do have knowledge of their future should be
 able to tell the scheduler about it early.
 Such insights enable the system to perform 
\begin_inset Flex SmallCaps
status collapsed

\begin_layout Plain Layout
look-ahead
\emph on
 
\emph default
scheduling
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
look-ahead scheduling
\end_layout

\end_inset

: It can make ahead-of-time, anticipating decisions rather than exercising
 post-mortem, reactive control.
\end_layout

\begin_layout Section
Thesis Goals
\end_layout

\begin_layout Standard
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 1
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
In this dissertation, I target three scheduling-related problems:
\end_layout

\begin_layout Itemize
\begin_inset Argument
status collapsed

\begin_layout Plain Layout
noitemsep
\end_layout

\end_inset

timeliness,
\end_layout

\begin_layout Itemize
efficient core placement, and
\end_layout

\begin_layout Itemize
quality-aware overload handling.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
I chose these problems because I believe they constitute important non-functiona
l properties
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
non-functional properties
\end_layout

\end_inset

, which applications should offer to their users.
 Therefore, I investigate these problems in the context of interactive end-user
 systems
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
interactive systems
\end_layout

\end_inset

, running a commodity operating system
\begin_inset CommandInset citation
LatexCommand cite
key "StatCounter:OSMarketShare"

\end_inset

 as an application platform.
 This category of systems includes classical desktop computers, notebooks
 and the ballooning family of smartphones and tablets.
 It does not include servers, although I am confident that my ideas generalize
 to this class of systems due to the large amount of shared technology.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
I do not consider reactive systems
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
reactive systems
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Harel:ReactiveSystems"

\end_inset

 that continuously supervise sensors and actuators, for example in an industrial
 control environment or within other deeply embedded systems.
 Graphical and touch user interfaces however are subject to timing constraints
 dictated by the physical reality, therefore human-machine-interfaces and
 reactive systems overlap.
\begin_inset CommandInset citation
LatexCommand cite
key "Halbwachs:LUSTRE"

\end_inset

 Beyond these common timeliness demands, I ignore reactive systems in this
 work.
\end_layout

\begin_layout Standard
Similarly, I do not consider offline scheduling, where a precomputed schedule
 is reenacted at runtime.
 In a dynamic, interactive system, advance knowledge on the executed task
 set is generally not available.
 Therefore, I exclusively research online scheduling, where scheduling decisions
 happen while the system runs.
\end_layout

\begin_layout Standard
\begin_inset Float marginfigure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\begin_inset Graphics
	filename Figures/1 Introduction/Deadline Types.svg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "mar:1-Deadline-Types"

\end_inset

Types of Real-Time Deadlines
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset VSpace defskip
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Real-time literature
\begin_inset CommandInset citation
LatexCommand cite
key "Liu:RealTimeSystems"

\end_inset

 distinguishes between systems with hard
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
hard deadline
\end_layout

\end_inset

, firm
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
firm deadline
\end_layout

\end_inset

 and soft deadlines
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
soft deadline
\end_layout

\end_inset

.
 Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "mar:1-Deadline-Types"

\end_inset

 shows the delineating characteristics.
 Hard deadlines must never be missed and the system has to guarantee this
 invariant with a formal analysis.
 For firm and soft deadlines, weaker guarantees apply.
 Deadlines may be missed, but with predictable consequences.
 Jobs missing a firm deadline are aborted, whereas results arriving after
 a soft deadline are still useful.
\end_layout

\begin_layout Standard
I think an interactive system should not reject the user’s instruction to
 start an application.
 Therefore, a task admission
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
admission
\end_layout

\end_inset

 is not appropriate and consequently, the system cannot handle hard deadlines.
 Overload situations may occur and my solution provides methods to handle
 them.
 It is up to the application to decide on aborting the job or continuing.
\end_layout

\begin_layout Standard
\noindent
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 1
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
Three layers have to be bridged
\end_layout

\end_inset

 for a comprehensive solution:
\end_layout

\begin_layout Itemize
\begin_inset Argument
status collapsed

\begin_layout Plain Layout
noitemsep
\end_layout

\end_inset

workload,
\end_layout

\begin_layout Itemize
application, and
\end_layout

\begin_layout Itemize
CPU scheduler.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
I presume that combining application-specific knowledge on workload and
 execution behavior with system-wide knowledge on urgency and overall load
 is beneficial.
 Any solution that handles any of these aspects in isolation will be incomplete.
 Instead, applications and the scheduler have to collaborate to integrate
 application-local and global scheduling mechanisms.
\end_layout

\begin_layout Standard
Furthermore, the three non-functional properties in the focus of this thesis
 interrelate: Core placement influences execution speed
\begin_inset CommandInset citation
LatexCommand cite
key "Blagodurov:DINO"

\end_inset

 and therefore application progress, which affects timeliness.
 When too tight timing requirements are requested for a single core to fulfill,
 parallel execution on multiple cores can help to meet demand, otherwise
 the system runs into overload.
 Unless mitigated, overload disturbs timeliness.
 The scheduling layer thus needs one mechanism covering all three of the
 non-functional properties.
 I propose self-describing jobs as this integrative device.
 Equally, application developers should not face three different programming
 paradigms, but one wholesale solution.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
Integration
\end_layout

\end_inset

 along the properties dimension and the layers dimension is key.
 The following Table
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "tab:1-Overview-Thesis"

\end_inset

 summarizes the thesis goals:
\end_layout

\begin_layout Standard
\begin_inset Float table
wide true
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="4">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="top" width="16text%">
<column alignment="left" valignment="top" width="23text%">
<column alignment="left" valignment="top" width="23text%">
<column alignment="left" valignment="top" width="23text%">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Timeliness
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
timeliness
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Core Placement
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Overload
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
overload
\end_layout

\end_inset

 Handling
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Workload
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
workload
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
derive workload metrics to predict execution times
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
balanced partitioning to enable parallel execution
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
offer degraded processing options
\end_layout

\end_inset
</cell>
</row>
<row bottomspace="default">
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Application
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
expose deadlines and execution time estimates ahead of time
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
deduce parallel speedup and workload metadata
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
balance resource and quality impact of load shedding
\end_layout

\end_inset
</cell>
</row>
<row bottomspace="default">
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CPU Scheduler
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
scheduler
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
globally order jobs to meet deadlines, detect misses ahead of time
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
non-work-conserving assignment of jobs to cores
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
exert back-pressure when anticipating deadline misses
\end_layout

\end_inset
</cell>
</row>
<row topspace="default" bottomspace="default">
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Enabling Feature
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Timing Requirements
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
timing requirements
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Resource Requirements
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
resource requirements
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Scheduler Look-Ahead
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
look-ahead scheduling
\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row bottomspace="default">
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Thesis Chapter
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Chapter
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "chap:3-Real-Time"

\end_inset

:
\begin_inset Newline newline
\end_inset


\begin_inset CommandInset ref
LatexCommand nameref
reference "chap:3-Real-Time"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Chapter
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "chap:4-Parallelism"

\end_inset

:
\begin_inset Newline newline
\end_inset


\begin_inset CommandInset ref
LatexCommand nameref
reference "chap:4-Parallelism"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Chapter
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "chap:5-Adaptivity"

\end_inset

:
\begin_inset Newline newline
\end_inset


\begin_inset CommandInset ref
LatexCommand nameref
reference "chap:5-Adaptivity"

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:1-Overview-Thesis"

\end_inset

Overview of the Solution Developed in this Thesis
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Vertical integration across the three layers is necessary, because no layer
 alone has enough knowledge to provide the desired non-functional properties.
 Horizontal integration across the properties is necessary to curb complexity
 for the developer and because of the cross-talk between the properties.
\end_layout

\begin_layout Standard
A large body of individual research results is available covering the nine
 intersections of my three by three goals matrix.
 However, in this dissertation I provide a comprehensive, end-to-end solution
 and I demonstrate improvements over the state of the art in each of the
 three property lanes.
 The scheduling system I present is dubbed 
\noun on
Atlas
\noun default
, the Auto-Training Look-Ahead Scheduler.
\begin_inset Foot
status open

\begin_layout Plain Layout
Like its namesake the greek titan who supports the celestial globe (cf.
\begin_inset space ~
\end_inset


\begin_inset CommandInset href
LatexCommand href
name "Wikipedia"
target "http://en.wikipedia.org/wiki/Atlas_(mythology)"

\end_inset

), we hope the 
\noun on
Atlas
\noun default
 system can support many applications.
\end_layout

\end_inset

 I will now give an overview of the work within each property lane and summarize
 the key contributions.
 Each lane is discussed in detail in its own chapter.
\end_layout

\begin_layout Section
Timeliness
\end_layout

\begin_layout Standard
Timeliness is the primary property of real-time scheduling.
 The scheduling policy considers secondary service goals only when timeliness
 is not jeopardized.
 Time is a global resource and is therefore managed by a system-wide scheduler.
 Imposing a separation of concerns as in the resource kernels concept,
\begin_inset CommandInset citation
LatexCommand cite
key "Rajkumar:ResourceKernels"

\end_inset

 applications specify their timing requirements to the scheduler, which
 then exercises global management.
 This functional separation must be accompanied by an integration of knowledge:
 Only the application knows its workload and should expose this knowledge
 to the scheduler using appropriate interfaces.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
I contribute a task model
\end_layout

\end_inset

 and a scheduler interface that allows applications to express timing and
 resource requirements.
 Other than the majority of related work, I do not employ a periodic task
 model
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
periodic task model
\end_layout

\end_inset

.
\begin_inset CommandInset citation
LatexCommand cite
key "Liu:RMS"

\end_inset

 Explicit submission of future jobs substitutes the implicit knowledge on
 future execution that periods provide.
\end_layout

\begin_layout Standard
The 
\noun on
Atlas
\noun default
 scheduler interface as seen by the application developer only asks for
 parameters from the application domain.
 Deadlines specify timing requirements.
 Resource requirements are specified using 
\begin_inset Flex SmallCaps
status collapsed

\begin_layout Plain Layout

\emph off
workload metrics
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
workload metrics
\end_layout

\end_inset

: parameters from the workload that describe its computational weight.
 
\noun on
Atlas
\noun default
 uses machine-learning to automatically derive execution time estimates
 from the metrics before the actual execution.
 The estimated execution time of each job is therefore known ahead of time.
 I presented this prediction method on the 27th IEEE Real-Time Systems Symposium.
\begin_inset CommandInset citation
LatexCommand cite
key "Roitzsch:Predict"

\end_inset


\end_layout

\begin_layout Standard
I demonstrate with code examples that this task model is easy to program
 against.
 I believe it is easier than reservation-based interfaces for which the
 developer needs to specify a desired CPU allocation.
 However, a formal usability analysis of the programming interface is not
 part of this thesis.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
I claim that the flexibility of this task model
\end_layout

\end_inset

 allows to inform the scheduler more accurately of application behavior
 than alternative approaches.
 I validate this claim by demonstrating that 
\noun on
Atlas
\noun default
 can predict the future execution of applications precise enough to anticipate
 deadline misses before they occur.
 Outside the implicit clairvoyance of periodic task systems, no other scheduling
 system I am aware of features a comparable look-ahead capability.
\end_layout

\begin_layout Standard
Literature mentions look-ahead together with scheduling in the areas of
 constraint satisfaction problems
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
constraint satisfaction problems
\end_layout

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "Schaerf:CSP"

\end_inset

 factory scheduling,
\begin_inset CommandInset citation
LatexCommand cite
key "Itoh:ProductionScheduling"

\end_inset

 and server management for on-demand video.
\begin_inset CommandInset citation
LatexCommand cite
key "Yu:VOD_LookAhead"

\end_inset

 For constraint satisfaction problems and factory scheduling, look-ahead
 improves the traversal of the solution search space.
 Thus, the scheduling does not look ahead along the time axis into the future,
 but along the search tree into the solution space.
 In the on-demand video context, the server tries to batch multiple viewers
 of the same video to save disk requests.
 Look-ahead and buffering help the server to satisfy multiple users from
 the same disk stream, even if they independently pause and resume playback.
 Patterson
\begin_inset space ~
\end_inset

et
\begin_inset space ~
\end_inset

al.
\begin_inset space ~
\end_inset

investigated the submission of future jobs to a peripheral scheduler to
 improve prefetching for IO
\begin_inset space ~
\end_inset

devices.
\begin_inset CommandInset citation
LatexCommand cite
key "Patterson:InformedPrefetching"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
I also evaluate the timeliness
\end_layout

\end_inset

 delivered by 
\noun on
Atlas
\noun default
 due to its compliance with the requested timing requirements.
 I compare with the behavior under conventional fair share scheduling without
 timing constraints.
 This evaluation serves to convince the reader of the scheduler’s basic
 functionality with respect to the timeliness property.
 I do not claim to improve the state of the art in this aspect.
\end_layout

\begin_layout Standard
The real-time task model and scheduler are described and evaluated in Chapter
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "chap:3-Real-Time"

\end_inset

.
\end_layout

\begin_layout Section
Core Placement
\end_layout

\begin_layout Standard
Managing multiple cores adds another dimension to the scheduling problem:
 Not only does the scheduler order jobs along the time axis, but it also
 has to decide on a placement of the work onto cores.
\end_layout

\begin_layout Standard
Serving the timeliness goal, applications structure their execution into
 jobs that bear a timing requirement.
 To enable parallel execution, applications must also structure their execution
 into independent pieces that can execute simultaneously.
 To simplify development, I decouple these two structures by allowing for
 parallelism within an individual job.
 Such task models have been presented before.
\begin_inset CommandInset citation
LatexCommand cite
after "-2\\baselineskip"
key "Collette:JobParallelism"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
manual offset
\end_layout

\end_inset


\end_layout

\begin_layout Standard
For jobs with internal parallelism, 
\noun on
Atlas
\noun default
 can trade using more cores against using more time on fewer cores.
 In order to make those decisions, it needs information about the available
 parallelism and speedup.
 Placement of work on cores also influences execution times due to contention
 on shared caches and memory.
\begin_inset CommandInset citation
LatexCommand cite
after "-3\\baselineskip"
key "Zhuravlev:DIO"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float marginfigure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\begin_inset Graphics
	filename Plots/1 Introduction/Parallel_Execution_Alternatives.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "mar:1-Parallel-Execution-Alternatives"

\end_inset

Parallel Execution Alternatives
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
I contribute a collaboration mechanism
\end_layout

\end_inset

 that allows applications to propagate the degree of parallelism to the
 scheduler.
 Integrating with the lambda programming-style, I tap into the application
 to automatically deduce available parallelism and to maintain a model of
 the parallel speedup
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
parallel speedup
\end_layout

\end_inset

: For each number of cores a job can make use of, the scheduler receives
 an execution time estimate as illustrated in Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "mar:1-Parallel-Execution-Alternatives"

\end_inset

.
 These alternative resource requirements are obtained ahead of time, allowing
 look-ahead scheduling.
\end_layout

\begin_layout Standard
Parallel speedup can be limited by serial sections in the applied algorithm
 or by imbalance in the workload partitioning.
 To demonstrate a typical optimization workflow, I show an example of improving
 the speedup by balancing the workload.
 I presented this work on the 7th International Conference on Embedded Software.
\begin_inset CommandInset citation
LatexCommand cite
key "Roitzsch:Balancing"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
I claim that the rich information
\end_layout

\end_inset

 provided by applications allows non-work-conserving scheduling
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
non-work-conserving scheduling
\end_layout

\end_inset

 disciplines that reduce core allocation more aggressively than competing
 approaches.
 A non-work-conserving scheduler deliberately keeps cores idle even though
 runnable threads are available.
 
\noun on
Atlas
\noun default
 knows about the resource requirements of jobs, so it can select a core
 assignment that uses a minimum number of cores while still meeting all
 timing requirements.
 The schedulers in commodity operating systems are work-conserving.
 They lack this knowledge and therefore occupy cores eagerly.
\end_layout

\begin_layout Standard
Because parallel speedup is typically sub-linear,
\begin_inset CommandInset citation
LatexCommand citet
key "Amdahl:AmdahlsLaw"

\end_inset

 reducing core allocation reduces the overhead that comes with parallel
 execution.
 The lowest overhead would be achieved with single-core execution, which
 however also yields the worst response time.
 Timing requirements constrain this optimization problem with a limit on
 the tolerated response time.
 Existing real-time research relies on the implicit knowledge of future
 jobs provided by periodic task models.
\begin_inset CommandInset citation
LatexCommand cite
key "Collette:JobParallelism"

\end_inset

 I show how look-ahead scheduling reduces core use compared to the worst-case
 planning periodic tasks imply.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
Non-work-conserving scheduling policies
\end_layout

\end_inset

 have been researched before to improve response times
\begin_inset CommandInset citation
LatexCommand cite
key "Rosti:NWC_Partitioning"

\end_inset

 or throughput.
\begin_inset CommandInset citation
LatexCommand cite
key "Fedorova:SMT_Scheduling"

\end_inset

 Keeping idle cores as spares to cater for future job arrivals reduces overall
 response times, however future arrivals are merely guessed from past observatio
ns.
 Throughput can suffer, when co-scheduled jobs causes thrashing on shared
 resources such as the last level cache.
 In such cases, deliberately not using all the available parallelism increases
 instruction throughput.
 A similar behavior can be observed with magnetic disks: Interleaving requests
 from two different applications can cause more disk-head movements and
 thus lower throughput compared to briefly idling the disk to allow for
 batching of consecutive requests from the same application.
\begin_inset CommandInset citation
LatexCommand cite
key "Iyer:Anticipatory"

\end_inset

 These solutions do not exploit knowledge of timing requirements, resource
 requirements or the benefit of look-ahead.
\end_layout

\begin_layout Standard
The parallel speedup estimation and the non-work-conserving scheduler are
 presented and discussed in Chapter
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "chap:4-Parallelism"

\end_inset

.
\end_layout

\begin_layout Section
Overload Handling
\end_layout

\begin_layout Standard

\noun on
Atlas
\noun default
 does not enforce an admission process.
 I think rejecting application launches or new subtasks within an application
 would be too surprising for both the user and the developer.
 Consequently, the system can experience overload situations.
 Lacking an admission that can analytically prevent overloads, 
\noun on
Atlas
\noun default
 instead needs to offer mechanisms that handle overload gracefully.
\end_layout

\begin_layout Standard
Overload can occur for two reasons: First, an application can lie about
 its CPU demand.
 It announces timing and resource requirements that the scheduler can meet
 with idle computing capacity.
 However, at runtime, the application’s jobs run longer than reported.
 The scheduler handles such situations by ensuring that lying applications
 never degrade the service of honest applications.
\end_layout

\begin_layout Standard
The second overload condition is more interesting: Applications specify
 their demands correctly, but collectively ask for more CPU resources than
 available.
 In such a situation, service degradation is unavoidable.
 The best the system can do is control the overload to maximize the quality
 delivered to the user.
 Quality is a workload-specific measure and applications may apply custom
 strategies to adapt to overload.
\begin_inset CommandInset citation
LatexCommand cite
key "Isovic:QoS_Video"

\end_inset

 Interaction between scheduler and applications is therefore needed.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
I contribute quality-weighted back-pressure
\end_layout

\end_inset


\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
back-pressure
\end_layout

\end_inset

 as a mechanism to communicate overload mitigation from the scheduler to
 applications.
 When the scheduler detects an overload situation, it forcibly reduces the
 CPU time allocated to each application so that the overload is resolved.
 These cutbacks are apportioned according to a quality-weighted policy
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
quality-weighted policy
\end_layout

\end_inset

: Applications tell the scheduler how much reduction they can tolerate to
 maintain a minimal acceptable quality.
 This quality threshold is resembles probabilistic real-time task models
 such as QRMS.
\begin_inset CommandInset citation
LatexCommand cite
after "-6\\baselineskip"
key "Hamann:QRMS"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
manual offset and a page flush
\end_layout

\end_inset

 The scheduler combines this elasticity with the CPU load caused by the
 application to distribute the cutbacks.
 I will evaluate how a fully quality-fair policy compares to a fully resource-fa
ir policy.
\end_layout

\begin_layout Standard
The remaining CPU time assigned to each application is reported back to
 encourage adaptation
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
adaptation
\end_layout

\end_inset

.
 Developers can implement custom degradation strategies like load-shedding
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
load-shedding
\end_layout

\end_inset

 or imprecise computation
\begin_inset Index idx
status collapsed

\begin_layout Plain Layout
imprecise computation
\end_layout

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
after "-4\\baselineskip"
key "Lin:Imprecise"

\end_inset

 that take an application-specific notion of quality into account.
 I published a quality-aware adaptation technique for video playback in
 the Journal of Visual Communication and Image Representation.
\begin_inset CommandInset citation
LatexCommand cite
after "-2\\baselineskip"
key "Roitzsch:VideoQuality"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
Global management and local adaptation
\end_layout

\end_inset

 to react to overload have been researched independently.
 Scheduler systems such as AQuoSA
\begin_inset CommandInset citation
LatexCommand cite
after "-1\\baselineskip"
key "Palopoli:AQuoSA"

\end_inset

 or Cooperative Polling
\begin_inset CommandInset citation
LatexCommand cite
key "Krasic:CoopPoll"

\end_inset

 manage overload situations by distributing CPU capacity according to an
 inter-application fairness policy.
 AQuoSA applies resource fairness, while Cooperative Polling uses application
 knowledge to establish quality fairness.
 Intra-application adaptation has been investigated for tasks like video
 playback
\begin_inset CommandInset citation
LatexCommand cite
key "Wuest:QoS_Video"

\end_inset

 or network servers.
\begin_inset CommandInset citation
LatexCommand cite
key "Welsh:SEDA"

\end_inset


\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Standard
Priority-Progress Adaptation
\begin_inset CommandInset citation
LatexCommand cite
key "Krasic:PriorityProgress"

\end_inset

 by Krasic
\begin_inset space ~
\end_inset

et
\begin_inset space ~
\end_inset

al.
 is the only work I am aware of that combines global overload management
 with per-application adaptation.
 It does so by exposing a quality measure for each individual job, allowing
 the scheduler to globally order jobs by quality.
 The adaptation strategy to shed low-priority jobs on overload is therefore
 dictated by the scheduling method.
\end_layout

\begin_layout Standard
None of the presented research results exploit look-ahead to steer adaptation
 decisions.
\end_layout

\begin_layout Standard
\begin_inset Flex NewThought
status collapsed

\begin_layout Plain Layout
I claim that look-ahead improves quality
\end_layout

\end_inset

 when mitigating overload.
 Because of its look-ahead characteristic, 
\noun on
Atlas
\noun default
 can detect overload before it occurs.
 Applications receive announcements of resource cutbacks ahead of time,
 including the time when the future deadline miss is anticipated and how
 much CPU capacity remains.
 Any pending job until the critical deadline can be degraded to satisfy
 the reduction in CPU time.
 The resulting choice allows trimming jobs with a favorable ratio between
 resource savings and quality impact.
 Additionally, the adaptation technique can be chosen by each application
 independently.
\end_layout

\begin_layout Standard
I validate the claim by comparing overload management with and without look-ahea
d.
 I also evaluate the flexibility of quality-weighted back-pressure by implementi
ng different degradation strategies.
\end_layout

\begin_layout Standard
Overload management and adaptation are explained and analyzed in Chapter
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "chap:5-Adaptivity"

\end_inset

.
\end_layout

\begin_layout Section
Demo Application
\end_layout

\begin_layout Standard
The primary example workload in this thesis is video playback.
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Itemize
introduce video as the primary demo application
\end_layout

\begin_deeper
\begin_layout Itemize
good sample workload, because it covers many properties: intrinsic deadlines,
 high throughput, parallel, potentially adaptive
\end_layout

\begin_layout Itemize
also: used in mobile scenarios, subject to energy savings
\end_layout

\begin_layout Itemize
this architecture also covers applications with just a subset of the properties
\end_layout

\begin_layout Itemize
video as running example, complemented with other tests to substantiate
 the results
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:2-Anatomy"

\end_inset

Anatomy of a Modern Desktop Application
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Itemize
major changes in application structure
\end_layout

\begin_deeper
\begin_layout Itemize
single-threaded
\end_layout

\begin_deeper
\begin_layout Itemize
UI blocks on every user-triggered activity
\end_layout

\begin_layout Itemize
unresponsive, not parallel, but simple code
\end_layout

\end_deeper
\begin_layout Itemize
multithreaded
\end_layout

\begin_deeper
\begin_layout Itemize
focus on responsiveness, do long-running work asynchronously off the UI
 thread
\end_layout

\begin_layout Itemize
threading is necessary, but not sufficient for concurrency: we saw that
 when moving to multicores
\end_layout

\begin_layout Itemize
explicit thread management with hard-coded or guessed thread count
\end_layout

\begin_layout Itemize
difficult code
\end_layout

\end_deeper
\begin_layout Itemize
today: lambdas
\end_layout

\begin_deeper
\begin_layout Itemize
the UI thread (main thread) does no real work (see microkernel paradigm),
 it merely dispatches (spine-analogy) to keep background workers busy
\end_layout

\begin_deeper
\begin_layout Itemize
similar to immediate/system reservations in resource kernels
\begin_inset CommandInset citation
LatexCommand cite
key "Rajkumar:ResourceKernels"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
similar to networks moving from switched circuits to packet routing
\end_layout

\begin_layout Itemize
analogy: packet-switching in the phone network, railway trains on a track
 network
\begin_inset CommandInset citation
LatexCommand cite
key "MacResearch:AboardGrandCentral"

\end_inset


\end_layout

\begin_layout Itemize
work set model
\begin_inset CommandInset citation
LatexCommand cite
key "Kulkarni:Galois"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
executing some job can add new jobs to the set
\end_layout

\begin_layout Itemize
useful to implement irregular algorithms (processing on graph-like data
 structures with pointers, opposite: dense array)
\end_layout

\end_deeper
\begin_layout Itemize
set down the terminology here:
\end_layout

\begin_deeper
\begin_layout Description
job: work item of code+metadata, unit of scheduling
\end_layout

\begin_layout Description
lambda: independently executable piece of code, unit of parallelism
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
implementations of lambda programming
\end_layout

\begin_deeper
\begin_layout Itemize
lambdas/closures/blocks/tasks/activities as useful abstraction for declaring
 a piece of work inline: code looks serial, logical locality
\end_layout

\begin_layout Itemize
Microsoft: 
\begin_inset Formula $\lambda$
\end_inset

-functions turn out to be the holy-grail feature
\begin_inset CommandInset citation
LatexCommand cite
key "Sutter:C++AMP_Keynote"

\end_inset

 for developing the parallel patterns library as a library
\end_layout

\begin_layout Itemize
four aspects: logical coherence, automatic state capture, asynchronous invocatio
n, automatic thread management
\end_layout

\begin_layout Itemize
closures solve the first two, work queues the other two
\end_layout

\begin_deeper
\begin_layout Itemize
keep pending packets, automatic thread assignment and pooling, system-wide
 thread balancing
\end_layout

\begin_layout Itemize
thread count becomes controllable
\end_layout

\begin_layout Itemize
goal: have the number of active threads match the number of (logical) cores
\begin_inset CommandInset citation
LatexCommand cite
key "Anderson:SchedulerActivations"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
do a table with checkmarks comparing different implementations of the paradigm
\end_layout

\begin_deeper
\begin_layout Itemize
Apple GCD, Microsoft PPL
\begin_inset CommandInset citation
LatexCommand cite
key "MSDN:C++PPL"

\end_inset

, X10
\begin_inset CommandInset citation
LatexCommand cite
key "Charles:X10"

\end_inset

: all four
\end_layout

\begin_layout Itemize
cross-thread signal-slot-connections in Qt
\begin_inset CommandInset citation
LatexCommand cite
key "Qt:SignalsSlotsThreads"

\end_inset

: asynchronous method calls, no automatic thread management, no automatic
 state-capturing
\end_layout

\begin_layout Itemize
Go-routines in Google Go: asynchronous method calls, automatic thread management
, no automatic state-capturing
\end_layout

\begin_layout Itemize
TaskC model for Core Manager
\begin_inset CommandInset citation
LatexCommand cite
key "Arnold:TaskC"

\end_inset

: asynchronous task spawning but not nested, automatic placement decision,
 manual statue capture because its not shared memory
\end_layout

\begin_layout Itemize
OpenMP for regular algorithms like iterating over dense arrays, mostly paralleli
zing loops: automatic thread assignment, guided self-scheduling to reduce
 dispatching overhead, synchronous
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
result of this architecture: most work is done asynchronously and is announced
 by the before execution
\end_layout

\begin_deeper
\begin_layout Itemize
limited view into the future as a common trait for modern applications (this
 is a key point here)
\end_layout

\begin_layout Itemize
describe fundamental video player architecture
\end_layout

\begin_deeper
\begin_layout Itemize
FFplay sets up a pipeline of three threads with very different characteristics
\end_layout

\begin_layout Itemize
input thread: blocks in IO, little CPU work
\end_layout

\begin_layout Itemize
decoder thread: high CPU throughput, highly dynamic
\end_layout

\begin_layout Itemize
output thread: very short, like a control task
\end_layout

\begin_layout Itemize
implementation is event-driven with thread-interdependencies
\end_layout

\begin_layout Itemize
spine: main thread runs event dispatch loop as is common in all GUI toolkits
\end_layout

\end_deeper
\begin_layout Itemize
lambdas (or groups of them) are natural receivers for metadata by the developer
 (deadlines) or automatically by the system (training)
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:3-Real-Time"

\end_inset

Real Simple Real
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
raisebox{-0.45mm}{-}
\end_layout

\end_inset

Time
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Itemize
many applications have intrinsic real-time characteristics
\end_layout

\begin_deeper
\begin_layout Itemize
real-time naturally surfaces when computers interact with the real world,
 in both input and output
\end_layout

\begin_layout Itemize
all desktops do that, yet almost no desktop software uses real-time programming
\end_layout

\begin_layout Itemize
deadlines for UI code driven by usability requirements: 10/100/1000
\begin_inset space \thinspace{}
\end_inset

ms
\end_layout

\begin_layout Itemize
obvious for animations, which become increasingly ubiquitous and which should
 not stutter if they are supposed to help usability by supporting mental
 transitions
\end_layout

\begin_layout Itemize
even harder for touch-based interaction, where lag of UI breaks the user's
 mental link of interacting with on-screen items directly
\end_layout

\begin_layout Itemize
often buried within the application and not expressed
\end_layout

\begin_layout Itemize
real-time used to be a rare special case, but it should become default;
 this work tries to pave the way
\end_layout

\end_deeper
\begin_layout Itemize
real-time scheduling is about ordering work so that it meets timing requirements
\end_layout

\begin_deeper
\begin_layout Itemize
order matters with time
\end_layout

\begin_layout Itemize
how to communicate timing and resource requirements between application
 and scheduler?
\end_layout

\begin_layout Itemize
existing solutions: spectrum between strong guarantees + complex programming
 and weak guarantees + simple programming
\end_layout

\begin_deeper
\begin_layout Itemize
points in this design space discussed in related work, here: only the ends
\end_layout

\end_deeper
\begin_layout Itemize
one end: classical periodic task model
\end_layout

\begin_deeper
\begin_layout Itemize
inflexible way to express timing requirements, pessimistic resource requirements
\end_layout

\begin_layout Itemize
fixed share of the CPU set aside
\end_layout

\begin_layout Itemize
for video: worst-case planning leads to infeasible reservation way off the
 actual demand
\end_layout

\begin_layout Itemize
however, the advantage: infinite clairvoyance allows an admission test
\end_layout

\begin_layout Itemize
strict admission not useful on desktops: yet another error condition for
 the developer, how to communicate refusal to the user?
\end_layout

\end_deeper
\begin_layout Itemize
other end: fair CPU sharing
\end_layout

\begin_deeper
\begin_layout Itemize
nothing to be done for developers, but also: nothing can be done
\end_layout

\begin_layout Itemize
scheduler distributes CPU evenly across applications according to a fairness
 measure
\end_layout

\begin_layout Itemize
users and applications do not want a fair share, so it's the wrong abstraction
\end_layout

\begin_layout Itemize
but it's the right abstraction for performance isolation
\end_layout

\begin_layout Itemize
pervasive: on Android all Dalvik threads use SCHED_OTHER
\begin_inset CommandInset citation
LatexCommand cite
key "Maia:AndroidRT"

\end_inset


\end_layout

\begin_layout Itemize
applications treated equally, disregarding their timing requirements because
 they are not expressed
\end_layout

\begin_layout Itemize
for video: concurrent heavy calculation may squeeze out video
\end_layout

\begin_layout Itemize
priorities would fix this, but they are an implementation detail of the
 scheduler that is hard to determine for developers in open systems
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
find a good middle ground between both approaches
\end_layout

\begin_deeper
\begin_layout Itemize
express timing requirements, but more flexible than periodic task: deadlines
\end_layout

\begin_layout Itemize
this is an attribute within the application's problem domain
\end_layout

\begin_deeper
\begin_layout Itemize
system cannot guess it
\end_layout

\begin_layout Itemize
developers should be able to provide it
\end_layout

\end_deeper
\begin_layout Itemize
express resource requirements, but without asking for execution times, which
 are hardware-dependent
\end_layout

\begin_layout Itemize
replace full clairvoyance with limited look-ahead
\end_layout

\begin_layout Itemize
instead of fairness and isolation first, put timeliness first and deal with
 fairness only when overloaded
\end_layout

\end_deeper
\begin_layout Itemize
deriving architecture from requirements
\end_layout

\begin_deeper
\begin_layout Itemize
execution time is workload-dependent, deadlines are application-specific:
 application-view needed
\end_layout

\begin_layout Itemize
scheduling must distribute loads so it meets all timing requirements: global
 view needed
\end_layout

\begin_layout Itemize
application-level component and global scheduler must work together
\end_layout

\end_deeper
\begin_layout Itemize
look-ahead
\end_layout

\begin_deeper
\begin_layout Itemize
for autonomous computing tasks, applications already know what lies ahead
\end_layout

\begin_deeper
\begin_layout Itemize
only code and data determine the upcoming computation, both are known ahead-of-t
ime
\end_layout

\begin_layout Itemize
counterexample: direct, low-latency reaction to user input
\end_layout

\end_deeper
\begin_layout Itemize
given the asynchronous execution model discussed above, this future execution
 can be exposed as real-time jobs early
\end_layout

\begin_deeper
\begin_layout Itemize
explain video player architecture with buffers and work submission (see
 status talk)
\end_layout

\begin_layout Itemize
individual lambdas may be too fine-grained
\end_layout

\begin_deeper
\begin_layout Itemize
model jobs as lambdas or groups thereof
\end_layout

\begin_layout Itemize
attach deadlines there
\end_layout

\begin_layout Itemize
GCD supports those
\end_layout

\end_deeper
\begin_layout Itemize
must stay within application domain
\end_layout

\begin_layout Itemize
metrics are needed to predict execution times
\end_layout

\end_deeper
\begin_layout Itemize
use this knowledge to provide insightful look-ahead
\end_layout

\begin_layout Itemize
previous work usually depends on black-box guessing and post-mortem control
\end_layout

\begin_deeper
\begin_layout Itemize
state the key advancement here and forward-reference to the full related
 work discussion
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
end-to-end solution
\end_layout

\begin_deeper
\begin_layout Itemize
workload: analyze dependency between data and execution time – execution
 time metrics for video
\begin_inset CommandInset citation
LatexCommand cite
key "Roitzsch:Predict"

\end_inset


\end_layout

\begin_layout Itemize
application: offer look into the future, derive execution time prediction
 from metrics, attach deadlines to jobs, propagate knowledge down to system
 level
\end_layout

\begin_layout Itemize
system: insert all jobs in an EDF run queue
\end_layout

\end_deeper
\begin_layout Itemize
explain online execution time training in detail
\end_layout

\begin_deeper
\begin_layout Itemize
evaluate accuracy of execution time prediction against worst-case allocation
 of a periodic task
\end_layout

\begin_layout Itemize
sensitivity analysis for all the 
\begin_inset Quotes eld
\end_inset

magic
\begin_inset Quotes erd
\end_inset

 parameters of the estimator
\end_layout

\end_deeper
\begin_layout Itemize
system scheduler
\end_layout

\begin_deeper
\begin_layout Itemize
job programming model not compatible with fixed task priorities
\end_layout

\begin_deeper
\begin_layout Itemize
the same thread might execute work of different urgency
\end_layout

\begin_layout Itemize
depends on what work the application dispatches when to what queue
\end_layout

\begin_layout Itemize
we could restrict the programmer to allow for more implicit scheduler knowledge
 (like minimum inter-arrival distance), but this work opts not to
\end_layout

\begin_layout Itemize
the urgency of one job remains the same over its lifetime
\end_layout

\begin_layout Itemize
we need dynamic task priorities, but have fixed job priorities, so EDF is
 a good start
\end_layout

\end_deeper
\begin_layout Itemize
our execution times are only estimates
\end_layout

\begin_deeper
\begin_layout Itemize
design for underestimation and late jobs
\end_layout

\begin_layout Itemize
late jobs should not interfere with honest jobs
\end_layout

\begin_layout Itemize
without overload, we want EDF-behavior even when all execution times are
 wrong (for example all 0)
\end_layout

\end_deeper
\begin_layout Itemize
in real applications, jobs may block
\end_layout

\begin_deeper
\begin_layout Itemize
blocking occurs when waiting for another resource
\end_layout

\begin_layout Itemize
we do not consider IO here, but only dependencies among different parts
 of the application
\end_layout

\begin_layout Itemize
video: producer-consumer scenario
\end_layout

\end_deeper
\begin_layout Itemize
scheduler design
\end_layout

\begin_deeper
\begin_layout Itemize
dispatch according to EDF among the not-late jobs
\end_layout

\begin_deeper
\begin_layout Itemize
without overload, there will be no late jobs
\end_layout

\begin_layout Itemize
gives us EDF-behavior when everyone behaves
\end_layout

\begin_layout Itemize
execution times not needed, so they can be arbitrarily wrong
\end_layout

\end_deeper
\begin_layout Itemize
round-robin all late jobs and the next job in EDF order when there is slack
\end_layout

\begin_deeper
\begin_layout Itemize
jobs that need more time can continue, but the next job in EDF order can
 also start work
\end_layout

\end_deeper
\begin_layout Itemize
run next job in EDF order exclusively when deadlines would otherwise be
 missed down the road
\end_layout

\begin_deeper
\begin_layout Itemize
uses estimated execution times to calculate deadline misses
\end_layout

\begin_layout Itemize
makes sure that late jobs do not interfere with the timeliness of honest
 jobs
\end_layout

\end_deeper
\begin_layout Itemize
create a virtual LRT schedule using the estimated execution times
\end_layout

\begin_deeper
\begin_layout Itemize
solves the blocking problem: when starting job at latest possible instant
 and it still blocks, there's nothing else we can do
\end_layout

\begin_layout Itemize
LRT is optimal
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
this scheduling behavior can be simulated in userland by setting thread
 priorities
\end_layout

\begin_deeper
\begin_layout Itemize
experimenting with close interaction of application and scheduler is easier
 in userland
\end_layout

\begin_layout Itemize
separating schedule look-ahead and actual dispatching would be a good implementa
tion strategy anyway
\end_layout

\begin_layout Itemize
only downside: overhead is an upper bound, might be smaller with an in-kernel
 implementation
\end_layout

\end_deeper
\begin_layout Itemize
background jobs can use infinite deadline, no extra best-effort scheduling-band
 needed
\end_layout

\begin_layout Itemize
deadline inheritance on blocking is left for others to analyze
\end_layout

\begin_layout Itemize
evaluate scheduler overhead
\end_layout

\end_deeper
\begin_layout Itemize
end-to-end evaluation
\end_layout

\begin_deeper
\begin_layout Itemize
illustrate that the scheduler works as intended
\end_layout

\begin_layout Itemize
timeliness against a fair share scheduler
\end_layout

\begin_layout Itemize
timeliness against CBS-style reservation
\end_layout

\begin_layout Itemize
system can predict deadline misses, even though it does not have implicit
 clairvoyance from periods
\end_layout

\end_deeper
\begin_layout Itemize
discuss related work in detail
\end_layout

\begin_deeper
\begin_layout Itemize
explore the spectrum introduced above, compare approaches to describe timing
 and resource requirements
\end_layout

\begin_layout Itemize
fixed-priority approaches: strongest guarantees possible, minimal implementation
\end_layout

\begin_deeper
\begin_layout Itemize
expose scheduler implementation to userland, other task models would be
 implemented on top
\end_layout

\begin_deeper
\begin_layout Itemize
microkernel bottom-up paradigm
\end_layout

\begin_layout Itemize
used in practice: QNX in the BlackBerry PlayBook
\end_layout

\end_deeper
\begin_layout Itemize
view shared with Nemesis
\begin_inset CommandInset citation
LatexCommand cite
key "Leslie:Nemesis"

\end_inset

: fixed priorities require full analysis of the system to determine the
 priorities
\end_layout

\begin_layout Itemize
periodic tasks with probabilistic admission
\begin_inset CommandInset citation
LatexCommand cite
key "Hamann:QAS,Hamann:QRMS"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
more flexible resource requirements than strict periodic tasks, probabilistic
 admission
\end_layout

\begin_layout Itemize
higher utilization, but still using period for timing requirements
\end_layout

\begin_layout Itemize
QRMS runs on top of fixed priority scheduling
\end_layout

\end_deeper
\begin_layout Itemize
RT in Linux
\end_layout

\begin_deeper
\begin_layout Itemize
SCHED_RR, SCHED_FIFO
\end_layout

\begin_layout Itemize
RT-patch adds SCHED_DEADLINE, previously SCHED_EDF
\begin_inset CommandInset citation
LatexCommand cite
key "Faggioli:SCHED_EDF"

\end_inset


\end_layout

\begin_layout Itemize
only for root processes
\end_layout

\begin_layout Itemize
no look-ahead
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
resource kernels
\begin_inset CommandInset citation
LatexCommand cite
key "Rajkumar:ResourceKernels"

\end_inset

 separate concerns top-down: applications specify, scheduler manages time
\end_layout

\begin_deeper
\begin_layout Itemize
similar insight into task model: requirements state explicitly, execution
 time is not portable, calibrate automatically
\end_layout

\begin_layout Itemize
otherwise very close to periodic tasks
\end_layout

\begin_layout Itemize
decouple different resources with buffering
\end_layout

\end_deeper
\begin_layout Itemize
reservation approaches: focus on isolation
\end_layout

\begin_deeper
\begin_layout Itemize
integration of hard real-time, soft real-time and best effort: RBED
\begin_inset CommandInset citation
LatexCommand cite
key "Brandt:RBED"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
focus in integration while preserving strong guarantees
\end_layout

\begin_layout Itemize
underlying hard real-time EDF scheduler, resource allocator on top protects
 EDF scheduler from overload
\end_layout

\begin_layout Itemize
key insight: separate resource management from resource dispatching
\end_layout

\begin_layout Itemize
starts with periodic task model and calculates relaxation bounds for parameters
 that keep the schedule feasible
\end_layout

\begin_layout Itemize
applications can submit jobs with parameters within those bounds
\end_layout

\begin_layout Itemize
set_rbed_scheduler() and rbed_deadline_met() calls similar to submit/next
\end_layout

\end_deeper
\begin_layout Itemize
Constant Bandwidth Server
\begin_inset CommandInset citation
LatexCommand cite
key "Abeni:CBS"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
based on the concept of processor reservations
\begin_inset CommandInset citation
LatexCommand cite
key "Mercer:CapacityReserve"

\end_inset


\end_layout

\begin_layout Itemize
employs EDF as the fundamental scheduler
\end_layout

\begin_layout Itemize
provides temporal isolation: protect hard real-time tasks from the messy
 soft real-time tasks
\end_layout

\begin_layout Itemize
different world view: illusion of a dedicated slower processor
\end_layout

\begin_layout Itemize
fluid flow allocation contradicts the job style, more amenable to threads
\end_layout

\begin_layout Itemize
server manages the deadlines (assignment, postponing), not the application
\end_layout

\begin_layout Itemize
does not target improving the soft-real time workload, thus somewhat orthogonal
 to this work
\end_layout

\begin_layout Itemize
could be used to provide isolated hard real-time load in our system
\end_layout

\end_deeper
\begin_layout Itemize
adaptive reservations
\begin_inset CommandInset citation
LatexCommand cite
key "Abeni:AdaptiveReservations"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
similarities: no worst-case planning, because it is wasteful; no focus on
 preventing every single deadline violation
\end_layout

\begin_layout Itemize
overload handling instead of admission
\end_layout

\begin_layout Itemize
two layer approach: dedicated controller per task like HDE
\end_layout

\begin_layout Itemize
dynamic bandwidth adaptation to workload as an afterthought
\end_layout

\begin_layout Itemize
black-box control vs.
 insightful cooperation using metrics
\end_layout

\begin_layout Itemize
my predictor can be used in a reservation-based system
\end_layout

\begin_layout Itemize
finishing work until the deadline matters, abstracting from the application's
 work pattern using reservations makes this harder to accomplish and requires
 more complexity in control layer
\end_layout

\begin_layout Itemize
CBS covers the real temporal behavior of the application, now we try to
 recreate that in the control layer
\end_layout

\end_deeper
\begin_layout Itemize
slack reclaiming added to CBS
\begin_inset CommandInset citation
LatexCommand cite
key "Palopoli:FeedbackReclaiming"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
results in AQuoSA
\begin_inset CommandInset citation
LatexCommand cite
key "Palopoli:AQuoSA"

\end_inset

 system
\end_layout

\end_deeper
\begin_layout Itemize
AIRS
\begin_inset CommandInset citation
LatexCommand cite
key "Kato:AIRS"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
CBS-based reservations combined with EDF-WM multicore scheduler
\end_layout

\begin_layout Itemize
reservation extends CBS to reclaim slack
\end_layout

\begin_layout Itemize
no evaluation against freely migrating schedulers such as CFS
\end_layout

\end_deeper
\begin_layout Itemize
Darwin time-constrained threads
\end_layout

\begin_deeper
\begin_layout Itemize
periodic thread with an average and maximum execution time
\end_layout

\begin_layout Itemize
accessible for non-root processes
\end_layout

\begin_layout Itemize
vague demotion policy to prevent denial of service
\end_layout

\begin_layout Itemize
execution time provided directly
\end_layout

\end_deeper
\begin_layout Itemize
Self-tuning scheduler
\begin_inset CommandInset citation
LatexCommand cite
key "Cucinotta:Self-Tuning"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
treat existing real-time application as black box: 
\begin_inset Quotes eld
\end_inset

applications that are characterized by some temporal constraints, but are
 not developed using a specific API
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
insight: applications have real-time behavior but do not expose it, I modify
 applications and allow them to expose this
\end_layout

\begin_layout Itemize
estimate period and execution time by looking at its outside behavior
\end_layout

\begin_layout Itemize
study in this paper: choosing the wrong scheduling period causes deadline
 misses or over-allocation, so schedulers that decouple scheduling period
 from application period can be inefficient
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
whole-system approaches: lightweight specification
\end_layout

\begin_deeper
\begin_layout Itemize
Redline
\begin_inset CommandInset citation
LatexCommand cite
key "Yang:Redline"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset CommandInset href
LatexCommand href
name "Redline Slides"
target "http://os.inf.tu-dresden.de/local/dropscon/archive/2008-12-03-Michael-Redline.pdf"

\end_inset


\end_layout

\begin_layout Itemize
also no modifications to applications
\end_layout

\begin_layout Itemize
manual annotation with period and fixed execution time
\end_layout

\begin_layout Itemize
will adapt parameters at runtime using feedback
\end_layout

\begin_layout Itemize
uses EDF scheduler
\end_layout

\end_deeper
\begin_layout Itemize
coop_poll
\begin_inset CommandInset citation
LatexCommand cite
key "Krasic:CoopPoll"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
uses application insight, no black-box approach, applications explicitly
 expose knowledge
\end_layout

\begin_layout Itemize
propagates application's task descriptions down to the kernel, but without
 look-ahead
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
fair processor sharing: minimal interface
\end_layout

\begin_deeper
\begin_layout Itemize
historical context: fairness between users time-sharing system
\end_layout

\begin_layout Itemize
good average application behavior when developers do nothing
\end_layout

\begin_layout Itemize
starts with no interface to expose anything
\end_layout

\begin_layout Itemize
virtual time approaches, like CFS
\begin_inset CommandInset citation
LatexCommand cite
key "Molnar:CFS"

\end_inset

 in Linux
\end_layout

\begin_layout Itemize
research approaches start out as fair sharing and add twists
\begin_inset CommandInset citation
LatexCommand cite
key "Bavier:VirtualTime"

\end_inset


\end_layout

\begin_layout Itemize
while coming from the other end, it overlaps with reservation approaches
 in the spectrum:
\end_layout

\begin_deeper
\begin_layout Itemize
sharing based on cycle rates with slack reclaiming similar to CBS work
\end_layout

\begin_layout Itemize
timing guarantees for virtual-time based algorithms are possible by showing
 equivalence to EDF scheduling
\end_layout

\end_deeper
\begin_layout Itemize
BVT
\begin_inset CommandInset citation
LatexCommand cite
key "Duda:BVT"

\end_inset

 adds a warp to express dispatch priority
\end_layout

\begin_deeper
\begin_layout Itemize
many task parameters (CPU share, warp time, warp time limit, warp time requireme
nt) require global knowledge, some dimensionless, not inherent to application
\end_layout

\begin_layout Itemize
warp time is a priority, admission control needed to control the assignment
 of warp parameters
\end_layout

\end_deeper
\begin_layout Itemize
BERT
\end_layout

\begin_deeper
\begin_layout Itemize
binary overload mitigation: important tasks unboundedly steal from unimportant
 tasks
\end_layout

\end_deeper
\begin_layout Itemize
Nemesis
\begin_inset CommandInset citation
LatexCommand cite
key "Leslie:Nemesis"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
criticism of virtual processors (fair sharing): performance of each virtual
 processor depends on load on other virtual processors
\end_layout

\begin_layout Itemize
both fixed priorities and virtual processors require global knowledge to
 predict their behavior
\end_layout

\begin_layout Itemize
basic idea: free developer from determining resource requirements
\end_layout

\begin_layout Itemize
use feedback control
\end_layout

\end_deeper
\begin_layout Itemize
like reservations, virtual time put isolation first and application’s timing
 requirements second
\end_layout

\begin_deeper
\begin_layout Itemize
my approach: put applications first, consider fairness only on overload
\end_layout

\begin_layout Itemize
without global coordination, applications should specify timing requirements
 directly, not as relative importance using weights or shares
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
task models that express other properties: gravitational task model
\end_layout

\begin_layout Itemize
two-level scheduling
\begin_inset CommandInset citation
LatexCommand cite
key "Shin:Compositional"

\end_inset


\end_layout

\begin_layout Itemize
device IO
\end_layout

\begin_deeper
\begin_layout Itemize
propagate deadlines to device queues when jobs issue IO requests
\end_layout

\begin_deeper
\begin_layout Itemize
helps anticipatory schedulers
\begin_inset CommandInset citation
LatexCommand cite
key "Iyer:Anticipatory"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
propagate future jobs to devices for prefetching
\begin_inset CommandInset citation
LatexCommand cite
key "Patterson:InformedPrefetching"

\end_inset


\end_layout

\begin_layout Itemize
GCD fosters asynchronous device access and queue-ahead of upcoming jobs:
 provides the same look-ahead for device IO
\end_layout

\begin_layout Itemize
devices become smart and self-schedule: manage the queue themselves
\end_layout

\begin_deeper
\begin_layout Itemize
example: interrupt rate limiting, low-latency packet override and another
 rate limiting for the low-latency override in Intel 82576 network card
\begin_inset CommandInset citation
LatexCommand cite
key "Intel_NIC"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Dynamic Active Subset helps
\begin_inset CommandInset citation
LatexCommand cite
key "Reuther:DAS"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
ARTIFACT
\begin_inset CommandInset citation
LatexCommand cite
key "Sasinowski:ARTIFACT"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
ubiquitous priorities, MPEG execution model hard, load-shedding (forward
 reference to adaptivity)
\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:4-Parallelism"

\end_inset

Scheduling meets Multicore
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Itemize
multicores add another scheduling dimension: placement in space accompanies
 ordering in time
\end_layout

\begin_layout Itemize
scheduling in time is being deemphasized
\end_layout

\begin_deeper
\begin_layout Itemize
context switching used to be practical when many threads needed to be multiplexe
d in quasi-parallel fashion on one CPU
\end_layout

\begin_layout Itemize
with the number of cores approaching the number of ready threads, this approach
 gets impractical
\end_layout

\begin_layout Itemize
micro-benchmarks show cache-miss-related slowdown caused by only the timer
 tick
\begin_inset CommandInset citation
LatexCommand cite
key "Tsafrir:OSNoise"

\end_inset


\end_layout

\begin_layout Itemize
hardware transactional memory aborts on context switch
\begin_inset CommandInset citation
LatexCommand cite
key "AMD_ASF_Spec"

\end_inset


\end_layout

\begin_layout Itemize
many other resources cannot be preempted at all
\end_layout

\begin_deeper
\begin_layout Itemize
disk jobs
\end_layout

\begin_layout Itemize
GPUs get preemption functionality, but there is a lot of state to save and
 restore
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
systems increasingly schedule in space instead of time
\end_layout

\begin_deeper
\begin_layout Itemize
parallelism is replacing frequency scaling as the means to increase performance
\end_layout

\begin_layout Itemize
Lampson predicted this
\begin_inset CommandInset citation
LatexCommand cite
key "Lampson:HintsDesign"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

split resources in a fixed way if in doubt, rather than sharing them
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
multiplex as long as hardware is expensive, statically assign once hardware
 is cheap
\end_layout

\end_deeper
\begin_layout Itemize
specialize cores dynamically to execute kernel services
\end_layout

\begin_layout Itemize
shutting down cores is also an important mechanism to save energy, because
 DVFS is losing effectiveness
\begin_inset CommandInset citation
LatexCommand cite
key "LeSueur:DVFS"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
work queues that are drained asynchronously support this well
\end_layout

\begin_deeper
\begin_layout Itemize
queues just offer parallelism, dispatching is separate, system can decide
 placement; threads blend this in one concept
\end_layout

\begin_layout Itemize
number of threads becomes controllable instead of being fixed in the application
 code
\end_layout

\begin_layout Itemize
work queues used within applications and also down to the kernel
\begin_inset CommandInset citation
LatexCommand cite
key "Soares:FlexSC"

\end_inset


\begin_inset Float marginfigure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Figures/Queues_Mode_Switches.svg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Work Queues Prevent Mode Switches
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
improves throughput and (surprisingly) also latency when loaded
\end_layout

\begin_layout Itemize
extends the spine+workers programming model to workloads where the kernel
 is the worker (i.e.
 Internet servers)
\end_layout

\begin_layout Itemize
notifications from kernel to user space could work asynchronously, but don't
 as far as I know, closest: kqueue
\begin_inset CommandInset citation
LatexCommand cite
key "Lemon:Kqueue"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
DSI on L4 provides asynchronous data movement between different address
 spaces
\end_layout

\begin_layout Itemize
many throughput-oriented drivers (network cards) work this way, too
\end_layout

\begin_layout Itemize
work queues also used to drive GPUs predictably
\begin_inset CommandInset citation
LatexCommand cite
key "Kato:Timegraph"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Tetris
\begin_inset Quotes erd
\end_inset

 style scheduling: we want to know the widths and heights of jobs to plan
 placement and order
\end_layout

\begin_deeper
\begin_layout Itemize
prerequisite to implementing placement policies is sufficient information
 on what you place
\end_layout

\begin_layout Itemize
using asynchronous blocks for parallelism and deadlines and metrics attached
 to block groups for real-time, both are decoupled
\end_layout

\begin_layout Itemize
execution time estimates give us the total execution time, but when spread
 across cores, it might take less wall-clock time to finish
\end_layout

\begin_layout Itemize
application must provide execution time estimate for any number of cores
 the job can use
\end_layout

\end_deeper
\begin_layout Itemize
end-to-end solution
\end_layout

\begin_deeper
\begin_layout Itemize
workload: make amenable for data-parallel processing like intrinsic load-balanci
ng
\begin_inset CommandInset citation
LatexCommand cite
key "Roitzsch:Balancing"

\end_inset


\end_layout

\begin_layout Itemize
application: offer parallelism to the system, provide estimates on parallel
 scalability
\end_layout

\begin_layout Itemize
system: non-work-conserving placement of jobs on cores and order in time
 based on deadlines, execution time estimates and parallelism
\end_layout

\end_deeper
\begin_layout Itemize
describe parallel workload and placement algorithm in detail
\end_layout

\begin_deeper
\begin_layout Itemize
video parallelization and balancing
\end_layout

\begin_layout Itemize
system communicates consequences of placement decision back to the application
\end_layout

\begin_deeper
\begin_layout Itemize
number of threads to be used by GCD runtime
\end_layout

\begin_layout Itemize
avoid context-switch, so no upcall interface, but info-page-style shared
 memory
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
system scheduler
\end_layout

\begin_deeper
\begin_layout Itemize
G-EDF is optimal for soft real-time
\begin_inset CommandInset citation
LatexCommand cite
key "Devi:GEDFTardiness"

\end_inset


\end_layout

\begin_layout Itemize
G-EDF not optimal with respect to feasibility, but optimal deadline-based
 algorithms are known
\begin_inset CommandInset citation
LatexCommand cite
key "Levin:DP-Fair"

\end_inset


\end_layout

\begin_layout Itemize
efficient EDF implementation strategies are known for single cores
\begin_inset CommandInset citation
LatexCommand cite
key "Short:EDFTaskManagement"

\end_inset

 and multicores
\begin_inset CommandInset citation
LatexCommand cite
key "Lelli:Scalable_EDF"

\end_inset


\end_layout

\begin_layout Itemize
extensive scalability studies for multicores
\begin_inset CommandInset citation
LatexCommand cite
key "Brandenburg:GlobalEDF"

\end_inset


\end_layout

\begin_layout Itemize
G-EDF scales for today's core counts, in the future we may want to consider
 clustered scheduling
\begin_inset CommandInset citation
LatexCommand cite
key "Calandrino:ClusteredEDF"

\end_inset


\end_layout

\begin_layout Itemize
ongoing discussion on trading schedulability and locality using partitioning,
 clustering
\begin_inset CommandInset citation
LatexCommand cite
key "Bastoni:EmpiricalComparison"

\end_inset

 and semi-partitioning
\begin_inset CommandInset citation
LatexCommand cite
key "Bastoni:SemiPartitioned"

\end_inset

 (some tasks allowed to migrate)
\end_layout

\end_deeper
\begin_layout Itemize
end-to-end evaluation
\end_layout

\begin_deeper
\begin_layout Itemize
non-work-conserving placement of jobs on one core as long as it fits, fire
 up second core only when necessary
\end_layout

\begin_layout Itemize
core count reduced more aggressively compared to:
\end_layout

\begin_deeper
\begin_layout Itemize
greedy commodity schedulers
\end_layout

\begin_layout Itemize
heuristics based on the past
\end_layout

\begin_layout Itemize
planning based on worst-case execution time and period
\begin_inset CommandInset citation
LatexCommand cite
key "Collette:JobParallelism"

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
discuss related work in detail
\end_layout

\begin_deeper
\begin_layout Itemize
schedule in space instead of in time
\end_layout

\begin_layout Itemize
OS design for systems with 1000+ cores focus on space sharing instead of
 time sharing
\begin_inset CommandInset citation
LatexCommand cite
key "Wentzlaff:fos"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
even stronger separation between cores: explicit communication instead of
 cache coherent shared memory
\begin_inset CommandInset citation
LatexCommand cite
key "Baumann:Barrelfish"

\end_inset


\end_layout

\begin_layout Itemize
run parts of the system close or on the device they target
\begin_inset CommandInset citation
LatexCommand cite
key "Nightingale:Helios"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
existing placement strategies can augment our placement algorithm
\end_layout

\begin_deeper
\begin_layout Itemize
fits well with our programming model of applications exposing metadata on
 parallel work
\end_layout

\begin_layout Itemize
performance counters often used for runtime monitoring, like online cache
 utility modeling
\begin_inset CommandInset citation
LatexCommand cite
key "West:CacheUtility"

\end_inset

 and detection of sharing patterns
\begin_inset CommandInset citation
LatexCommand cite
key "Tam:SharingAwareScheduling"

\end_inset


\end_layout

\begin_layout Itemize
L2 cache misses is the primary metric that expresses locality of a job,
 because it covers the two major harmful communication paths: memory accesses
 and coherency traffic
\end_layout

\begin_layout Itemize
placement of jobs on cores influences contention on caches, prefetcher and
 DRAM controller
\begin_inset CommandInset citation
LatexCommand cite
key "Zhuravlev:DIO"

\end_inset


\end_layout

\begin_layout Itemize
cache thrashing with hyperthreads
\begin_inset CommandInset citation
LatexCommand cite
key "Fedorova:SMT_Scheduling"

\end_inset


\end_layout

\begin_layout Itemize
on NUMA-systems, placement influences memory latency
\begin_inset CommandInset citation
LatexCommand cite
key "Blagodurov:DINO"

\end_inset


\end_layout

\begin_layout Itemize
batch work by moving it around in the scheduling window to enable longer
 sleep times
\begin_inset CommandInset citation
LatexCommand cite
key "Awan:EnhancedRaceToHalt"

\end_inset


\end_layout

\begin_layout Itemize
migrate to prevent fan noise or throttling
\begin_inset CommandInset citation
LatexCommand cite
key "Merkel:EnergyModel"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
the execution order of blocks within a job is relevant for speed and cache
 working set size
\begin_inset CommandInset citation
LatexCommand cite
key "Chen:ConstructiveCacheSharing"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
use a lightweight specification language to describe beneficial scheduling
 policies
\begin_inset CommandInset citation
LatexCommand cite
key "Nguyen:ConcurrentSchedulers"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
knowledge about the job layout in both space and time allows disabling the
 periodic timer tick
\end_layout

\begin_deeper
\begin_layout Itemize
dynamically dedicate cores: place work somewhere, then let it run without
 interference
\end_layout

\begin_deeper
\begin_layout Itemize
the timer-tick is OS-level polling, hits all cores no matter what they run
\end_layout

\begin_layout Itemize
move to a tick-less kernels helps to reducing OS noise and energy consumption
\end_layout

\end_deeper
\begin_layout Itemize
good for barrier-sync-style HPC applications: even the timer interrupt hurts
 them badly
\begin_inset CommandInset citation
LatexCommand cite
key "Tsafrir:OSNoise"

\end_inset


\end_layout

\begin_layout Itemize
suggested solution: demand-driven 
\begin_inset Quotes eld
\end_inset

smart
\begin_inset Quotes erd
\end_inset

 timers which allow for batching instead of the slavish periodic tick
\end_layout

\begin_deeper
\begin_layout Itemize
idea is an extension to timers with precision
\begin_inset CommandInset citation
LatexCommand cite
key "Peter:30Seconds"

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
hardware extensions supporting work queues and reduced context switching
\end_layout

\begin_deeper
\begin_layout Itemize
NVIDIA Fermi uses GigaThread hardware to decide placement
\begin_inset CommandInset citation
LatexCommand cite
key "NVIDIA_Fermi"

\end_inset


\end_layout

\begin_layout Itemize
Core Manager distributes work and its data to processing elements
\begin_inset CommandInset citation
LatexCommand cite
key "Limberg:Tomahawk"

\end_inset


\end_layout

\begin_layout Itemize
Asynchronous Direct Messages
\begin_inset CommandInset citation
LatexCommand cite
key "Sanchez:ADM"

\end_inset

 allow very fine-grained lambdas
\end_layout

\begin_deeper
\begin_layout Itemize
more flexible: small hardware extension to be used by work queue implementation
\end_layout

\begin_layout Itemize
allows exposing more parallelism by splitting into smaller lambdas
\end_layout

\begin_layout Itemize
scalable because of local queues and work stealing that bypasses the memory
 hierarchy
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
holistic approach that extends placement problems beyond individual computers
 to distributed systems: MOSIX
\end_layout

\begin_deeper
\begin_layout Itemize
load balancing on networks of computers, predicts future problems within
 manycores
\end_layout

\begin_layout Itemize
increasing message latency between cores relative to compute speed: placement
 of work and communication patterns relevant
\end_layout

\begin_layout Itemize
employs gossiping to establish global system view without a central authority
\end_layout

\begin_layout Itemize
ad-hoc solution for my approach: assume that lambdas within the same job
 are more likely to talk to each other, place closer together
\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:5-Adaptivity"

\end_inset

Adapt to Handle Overload
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Itemize
our 
\begin_inset Quotes eld
\end_inset

no admission
\begin_inset Quotes erd
\end_inset

 policy requires us to deal with overloads when the work does not fit within
 the deadlines
\end_layout

\begin_deeper
\begin_layout Itemize
but only when overloaded: timeliness first, fairness only when overloaded
\end_layout

\end_deeper
\begin_layout Itemize
load shedding is an old concept
\begin_inset CommandInset citation
LatexCommand cite
key "Lampson:HintsDesign"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

shed load to control demand, rather than allowing the system to become overloade
d.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
Robert Morris idea: red button next to terminal, push when dissatisfied,
 system must raise your QoS or throw you off
\end_layout

\begin_layout Itemize
this old idea already argued for quality-aware adaptation, not just load-aware
\end_layout

\begin_layout Itemize
when overloaded, real-time applications become brittle and show bi-modal
 fairness
\begin_inset CommandInset citation
LatexCommand cite
key "Krasic:PriorityProgress"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
quality-aware adaptation is application-specific
\end_layout

\begin_deeper
\begin_layout Itemize
some work just needs to be done, no discussion, no adaptation possible
\end_layout

\begin_deeper
\begin_layout Itemize
I would argue this is the common case
\end_layout

\begin_layout Itemize
we can only hope to compensate by using the most efficient compute resource
 and organize it the most efficient way
\end_layout

\end_deeper
\begin_layout Itemize
some applications can make quality/resource trade-offs according to application-
specific notion of quality
\end_layout

\begin_deeper
\begin_layout Itemize
video is one example
\end_layout

\begin_layout Itemize
however, quality per invested CPU time is varying: with resource-fairness,
 quality would be varying
\end_layout

\begin_layout Itemize
application knowledge needed, however applications can lie to unfairly degrade
 others
\end_layout

\begin_layout Itemize
combine resource-fairness and quality-fairness
\end_layout

\begin_layout Itemize
this also applies to resources other than CPU time, like network bandwidth
 for streaming
\end_layout

\end_deeper
\begin_layout Itemize
look-ahead enables choosing the part to degrade amongst many options, compared
 to forcibly degrade the job at hand
\end_layout

\end_deeper
\begin_layout Itemize
end-to-end solution
\end_layout

\begin_deeper
\begin_layout Itemize
workload: offer different processing paths, trading resources and quality
\end_layout

\begin_layout Itemize
application: under given resource constraints, find a quality-optimal path
 through this lattice of options
\end_layout

\begin_layout Itemize
system: distribute computing resource cutbacks in a quality-fair fashion
\end_layout

\end_deeper
\begin_layout Itemize
backpressure needs to be quality-fair, not resource-fair, but the quality
 scale is application specific and does not compare
\end_layout

\begin_deeper
\begin_layout Itemize
global quality-scheduling too complex, resort to balanced backpressure and
 per-application adaptivity
\end_layout

\begin_layout Itemize
needs help from the applications: provide percentage of missed deadlines
 relative to the maximum quality case, that represents the bare minimum
 quality users will accept
\end_layout

\begin_layout Itemize
backpressure will be distributed amongst adaptive applications weighted
 by this minimum quality
\end_layout

\begin_layout Itemize
assumption: linear quality drop from meeting all deadlines down to that
 percentage
\end_layout

\begin_deeper
\begin_layout Itemize
could also use time-utility functions to optimize
\end_layout

\begin_layout Itemize
could also include heuristics like less degradation for the frontmost applicatio
n
\end_layout

\end_deeper
\begin_layout Itemize
use of percentage of missed deadlines as quality parameter inspired by QRMS
\begin_inset CommandInset citation
LatexCommand cite
key "Hamann:QRMS"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
describe fallback decoding path for video
\begin_inset CommandInset citation
LatexCommand cite
key "Roitzsch:VideoQuality"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
decoding time estimate already given, quality estimate needed
\end_layout

\begin_layout Itemize
greedy solution for (constrained) bin-packing: order jobs by quality/resource
 ratio
\end_layout

\end_deeper
\begin_layout Itemize
end-to-end evaluation
\end_layout

\begin_deeper
\begin_layout Itemize
compare reduction policy in scheduler: quality-based vs.
 fairness-based
\end_layout

\begin_layout Itemize
compare to ad-hoc adaptation using a low water-mark in the video player
 queue
\end_layout

\begin_layout Itemize
compare to fully clairvoyant oracle that calculates truly quality-optimal
 execution
\end_layout

\end_deeper
\begin_layout Itemize
discuss related work
\end_layout

\begin_deeper
\begin_layout Itemize
task models that allow for deadline misses
\end_layout

\begin_deeper
\begin_layout Itemize
(m,k)-firm tasks
\end_layout

\begin_layout Itemize
imprecise computation
\begin_inset CommandInset citation
LatexCommand cite
key "Lin:Imprecise"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
scheduling systems that control overload
\end_layout

\begin_deeper
\begin_layout Itemize
overload handling with adaptive reservations
\begin_inset CommandInset citation
LatexCommand cite
key "Abeni:AdaptiveReservations"

\end_inset


\end_layout

\begin_layout Itemize
virtual time for progress fairness
\begin_inset CommandInset citation
LatexCommand cite
key "Krasic:CoopPoll"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
resource kernels
\begin_inset CommandInset citation
LatexCommand cite
key "Rajkumar:ResourceKernels"

\end_inset

: user-level QoS expected to be more useful than stacked schedulers
\end_layout

\begin_layout Itemize
application-specific adaptation
\end_layout

\begin_deeper
\begin_layout Itemize
load-shedding queues in network servers
\begin_inset CommandInset citation
LatexCommand cite
key "Welsh:SEDA"

\end_inset


\end_layout

\begin_layout Itemize
reducing video quality
\begin_inset CommandInset citation
LatexCommand cite
key "Isovic:QoS_Video,Wuest:QoS_Video"

\end_inset


\end_layout

\begin_layout Itemize
other sources for adaptive video
\end_layout

\begin_deeper
\begin_layout Itemize
SVC
\end_layout

\begin_layout Itemize
adaptive streaming protocols: application propagates back-pressure even
 to the server
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
prioritize work and skip the low priorities when congested
\begin_inset CommandInset citation
LatexCommand cite
key "Krasic:PriorityProgress"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
simple quality-aware scheme
\end_layout

\begin_layout Itemize
global priority-monotonic ordering of video frames
\end_layout

\begin_layout Itemize
unclear how much of the fairness benefits stem from the priority sorting
 or just from global queueing
\end_layout

\end_deeper
\end_deeper
\end_inset


\end_layout

\begin_layout Chapter
The Road Ahead
\end_layout

\begin_layout Standard
\begin_inset Note Greyedout
status open

\begin_layout Itemize
changing paradigm of computing, future: always on, always connected
\end_layout

\begin_deeper
\begin_layout Itemize
smartphones, tablets and the cloud usher in a new era
\end_layout

\begin_layout Itemize
mobile computing, but still with responsiveness and smooth video demanded
 by users
\end_layout

\begin_layout Itemize
battery and energy constraints added to the problem space
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

energy is the new speed
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
energy works differently
\end_layout

\begin_deeper
\begin_layout Itemize
deadlines are inherent to the application, energy limitations are driven
 by the long-term intent of the user
\end_layout

\begin_layout Itemize
many existing solutions throttle threads according to an energy budget
\begin_inset CommandInset citation
LatexCommand cite
key "Roy:Cinder"

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
not graceful degradation, but just degradation
\end_layout

\end_deeper
\begin_layout Itemize
assuming software does only useful work against the user's stated intent,
 throttling makes no sense
\end_layout

\begin_layout Itemize
only energy efficiency (doing the same with less energy) and maybe adaptivity
 remains
\end_layout

\begin_deeper
\begin_layout Itemize
energy saving by batching is already enabled by look-ahead deadline-knowledge
\end_layout

\begin_layout Itemize
batch disk IO with deferrable requests
\begin_inset CommandInset citation
LatexCommand cite
key "Weissel:Coop_IO"

\end_inset


\end_layout

\begin_layout Itemize
batch network requests to power up radio less often
\begin_inset CommandInset citation
LatexCommand cite
key "Roy:Cinder"

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
we may have targeted hardware with reduced instruction sets available that
 is more energy-efficient
\end_layout

\begin_deeper
\begin_layout Itemize
pieces of GPU code (and other ISAs with tighter assumptions and less ability)
 interwoven with CPU code
\end_layout

\begin_layout Itemize
devices like GPU (and increasingly network cards
\begin_inset CommandInset citation
LatexCommand cite
key "Nightingale:Helios"

\end_inset

) treated as compute resources like CPU, not as a peripheral
\end_layout

\begin_layout Itemize
hardware video decoders as an example of a special-purpose, limited-ISA
 co-processor
\end_layout

\begin_deeper
\begin_layout Itemize
saves some energy (up to 25%
\begin_inset CommandInset citation
LatexCommand cite
key "LeSueur:SlowDownSleep"

\end_inset

), but not game-changing savings
\end_layout

\begin_layout Itemize
treat as any coprocessor that work with tighter restrictions on the available
 instructions can be offloaded to (see 
\family typewriter
restrict
\family default
 keyword proposal
\begin_inset CommandInset citation
LatexCommand cite
key "Sutter:C++AMP_Keynote"

\end_inset

)
\end_layout

\end_deeper
\begin_layout Itemize
special-purpose devices offer more choice for job placement and change energy
 tradeoffs, but they do not eliminate the fundamental problem to get given
 work done (meeting given deadlines) with as little resources (energy, time)
 as possible
\end_layout

\begin_layout Itemize
choosing between race-to-halt and slowdown depends on workload and platform
\begin_inset CommandInset citation
LatexCommand cite
key "LeSueur:SlowDownSleep"

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
end-to-end solution: this is only a rough sketch
\end_layout

\begin_deeper
\begin_layout Itemize
workload: energy-aware encoding of video
\end_layout

\begin_layout Itemize
we get the three swim lanes again, but with a different metric and intention
\end_layout

\begin_layout Itemize
application: maintain energy model (enhances real-time), code for specialized
 hardware ISA (enables placement), adapt to save energy (enhances adaptivity)
\end_layout

\begin_layout Itemize
system: collect and act on energy metrics (enhances real-time), outsource
 to specialized hardware (enhances placement), backpressure based on energy
 intent (enhances adaptivity)
\end_layout

\end_deeper
\begin_layout Itemize
metadata to decide energy consequences can be collected per-job at runtime
\end_layout

\begin_deeper
\begin_layout Itemize
performance counter can help to tell memory- and CPU-bound jobs apart
\end_layout

\begin_deeper
\begin_layout Itemize
clocking down the CPU especially helpful for memory-bound jobs
\begin_inset CommandInset citation
LatexCommand cite
key "Snowdon:Koala"

\end_inset


\end_layout

\begin_layout Itemize
exploit look-ahead for energy metadata: clock CPU down and then run all
 pending memory-bound jobs
\end_layout

\end_deeper
\begin_layout Itemize
linear combination of metrics to predict energy in a similar way to the
 execution time
\end_layout

\begin_deeper
\begin_layout Itemize
performance counters to train energy model
\begin_inset CommandInset citation
LatexCommand cite
key "Merkel:EnergyModel"

\end_inset


\end_layout

\begin_layout Itemize
energy counter in Intel Sandy Bridge CPUs
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
using backpressure also for energy-management
\end_layout

\begin_deeper
\begin_layout Itemize
some applications can adapt their energy or computation time needs
\begin_inset CommandInset citation
LatexCommand cite
key "Flinn:EnergyAdaptation"

\end_inset


\end_layout

\begin_layout Itemize
trivial examples (disable backup on low battery) are rather obvious and
 not interesting, more dynamic examples are rare
\end_layout

\begin_layout Itemize
use energy instead of CPU time as a cost metric in adaptation decisions
\end_layout

\end_deeper
\begin_layout Itemize
future development can be researched within the presented design
\end_layout

\begin_layout Itemize
summarize the key contributions again
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
backmatter
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Bibliography/Thesis"
options "Bibliography/Thesis"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nochapteredge
\end_layout

\end_inset


\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList margintable

\end_inset


\begin_inset CommandInset index_print
LatexCommand printindex
type "idx"

\end_inset


\end_layout

\end_body
\end_document
